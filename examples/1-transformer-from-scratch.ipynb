{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Scratch\n",
    "\n",
    "Reimplementation of Transformer from Scratch using JAX and tx, original notebook by Callum McDougall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:24.926875362Z",
     "start_time": "2023-09-10T05:26:24.869677090Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from jax.config import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Inputs and Outputs of a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:25.403495857Z",
     "start_time": "2023-09-10T05:26:24.925717866Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import re\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import einops\n",
    "from jaxtyping import Array, Float, Int\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "import tx\n",
    "from tx.hooks import Hook, HookPoint, HookMap\n",
    "from tx.models import PretrainedGPT2Model\n",
    "from tx.network import GenerativeModel\n",
    "\n",
    "from params import (\n",
    "    tfs_layer_norm_params,\n",
    "    tfs_embed_params,\n",
    "    tfs_pos_embed_params,\n",
    "    tfs_attention_params,\n",
    "    tfs_mlp_params,\n",
    "    tfs_block_params,\n",
    "    tfs_unembed_params,\n",
    "    tfs_transformer_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.087644665Z",
     "start_time": "2023-09-10T05:26:25.405309541Z"
    }
   },
   "outputs": [],
   "source": [
    "def store_as_state_hook(\n",
    "    x: Array,\n",
    "    hook_point: HookPoint = None,\n",
    "    module: nn.Module = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if module is not None and hook_point is not None:\n",
    "        module.sow(\"intermediates\", hook_point.value, x)\n",
    "    return x\n",
    "\n",
    "\n",
    "hooks = HookMap({\n",
    "    HookPoint.RESIDUAL.value: Hook(store_as_state_hook),\n",
    "    HookPoint.FINAL_OUTPUT.value: Hook(store_as_state_hook),\n",
    "    HookPoint.LN_NORMALIZED.value: Hook(store_as_state_hook),\n",
    "})\n",
    "\n",
    "\n",
    "reference_gpt2 = GenerativeModel(\n",
    "    config=PretrainedGPT2Model.tx_config,\n",
    "    params=PretrainedGPT2Model.from_pretrained(\"gpt2\").to_params(),\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained(\"gpt2\"),\n",
    "    hooks=hooks,\n",
    "    hook_collections=[\"intermediates\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.166481780Z",
     "start_time": "2023-09-10T05:26:32.164941362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19)]\n",
      "\n",
      "[('ľ', 250), ('Ŀ', 251), ('ŀ', 252), ('Ł', 253), ('ł', 254), ('Ń', 255), ('Ġt', 256), ('Ġa', 257), ('he', 258), ('in', 259), ('re', 260), ('on', 261), ('Ġthe', 262), ('er', 263), ('Ġs', 264), ('at', 265), ('Ġw', 266), ('Ġo', 267), ('en', 268), ('Ġc', 269)]\n",
      "\n",
      "[('Ġprodu', 990), ('Ġstill', 991), ('led', 992), ('ah', 993), ('Ġhere', 994), ('Ġworld', 995), ('Ġthough', 996), ('Ġnum', 997), ('arch', 998), ('imes', 999), ('ale', 1000), ('ĠSe', 1001), ('ĠIf', 1002), ('//', 1003), ('ĠLe', 1004), ('Ġret', 1005), ('Ġref', 1006), ('Ġtrans', 1007), ('ner', 1008), ('ution', 1009)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab = sorted(\n",
    "    list(reference_gpt2.tokenizer.get_vocab().items()), key=lambda n: n[1]\n",
    ")\n",
    "print(sorted_vocab[:20])\n",
    "print()\n",
    "print(sorted_vocab[250:270])\n",
    "print()\n",
    "print(sorted_vocab[990:1010])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.170814861Z",
     "start_time": "2023-09-10T05:26:32.166617496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Revolution', 50237), ('Ġsnipers', 50238), ('Ġreverted', 50239), ('Ġconglomerate', 50240), ('Terry', 50241), ('794', 50242), ('Ġharsher', 50243), ('Ġdesolate', 50244), ('ĠHitman', 50245), ('Commission', 50246), ('Ġ(/', 50247), ('âĢ¦.\"', 50248), ('Compar', 50249), ('Ġamplification', 50250), ('ominated', 50251), ('Ġregress', 50252), ('ĠCollider', 50253), ('Ġinformants', 50254), ('Ġgazed', 50255), ('<|endoftext|>', 50256)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_vocab[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.232348717Z",
     "start_time": "2023-09-10T05:26:32.171390476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'R', 'alph']\n",
      "['<|endoftext|>', ' Ralph']\n",
      "['<|endoftext|>', ' r', 'alph']\n",
      "['<|endoftext|>', 'ral', 'ph']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_list(\"Ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\" Ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\" ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\"ralph\", prepend_bos=True, truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.272518593Z",
     "start_time": "2023-09-10T05:26:32.231252538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['568', '73', '+', '318', '46', '23', '=', '123', '45', '67', '89', '-', '1', '000000', '000']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_list(\"56873+3184623=123456789-1000000000\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.359439447Z",
     "start_time": "2023-09-10T05:26:32.274652184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256 15496    11   314   716]\n",
      "(5,)\n",
      "['<|endoftext|>', 'Hello', ',', ' I', ' am']\n"
     ]
    }
   ],
   "source": [
    "# reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "reference_text = \"Hello, I am\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text, prepend_bos=True)\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_list(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.022075741Z",
     "start_time": "2023-09-10T05:26:32.358809253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "mask.shape (1, 5, 5)\n",
      "mask [[[ True False False False False]\n",
      "  [ True  True False False False]\n",
      "  [ True  True  True False False]\n",
      "  [ True  True  True  True False]\n",
      "  [ True  True  True  True  True]]]\n",
      "(5, 50257)\n"
     ]
    }
   ],
   "source": [
    "logits, state = reference_gpt2(tokens)\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.111822337Z",
     "start_time": "2023-09-10T05:26:33.024667605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 50257)\n"
     ]
    }
   ],
   "source": [
    "probs: Array = jax.nn.softmax(logits, axis=-1)\n",
    "print(probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.162448945Z",
     "start_time": "2023-09-10T05:26:33.117037794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<|endoftext|>', '\\n'), ('Hello', ','), (',', ' I'), (' I', \"'m\"), (' am', ' a')]\n"
     ]
    }
   ],
   "source": [
    "most_likely_next_tokens = reference_gpt2.to_str_list(jnp.argmax(logits, axis=-1))\n",
    "print(list(zip(reference_gpt2.to_str_list(tokens), most_likely_next_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.205778360Z",
     "start_time": "2023-09-10T05:26:33.162027189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' a'\n"
     ]
    }
   ],
   "source": [
    "next_token = jnp.argmax(logits[-1], axis=-1, keepdims=True)\n",
    "next_char = reference_gpt2.to_str(next_token)\n",
    "print(repr(next_char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.206099232Z",
     "start_time": "2023-09-10T05:26:33.205328282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>Hello, I am a student at the University of California, Berkeley."
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str(tokens), end=\"\", flush=True)\n",
    "\n",
    "for i in range(10):\n",
    "    print(next_char, end=\"\", flush=True)\n",
    "    # Define new input sequence, by appending the previously generated token\n",
    "    tokens = jnp.concatenate([tokens, next_token], axis=-1)\n",
    "    # # Pass our new sequence through the model, to get new output\n",
    "    logits, _ = reference_gpt2(tokens)\n",
    "    # # Get the predicted token at the end of our sequence\n",
    "    next_token = jnp.argmax(logits[-1], axis=-1, keepdims=True)\n",
    "    # # Decode and print the result\n",
    "    next_char = reference_gpt2.to_str(next_token)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_0:\n",
      "  ln_1:\n",
      "    normalized_hook: (35, 768).\n",
      "  ln_2:\n",
      "    normalized_hook: (35, 768).\n",
      "residual_hook: (35, 768).\n",
      "ln_f:\n",
      "  normalized_hook: (35, 768).\n",
      "final_output_hook: (35, 768).\n"
     ]
    }
   ],
   "source": [
    "def print_nicely(x, indent=0):\n",
    "    if isinstance(x, dict):\n",
    "        for k, v in x.items():\n",
    "            matches = re.findall(r\"block_(\\d+)\", k)\n",
    "            if matches and matches[0] != \"0\":\n",
    "                continue\n",
    "\n",
    "            print(f\"{'  ' * indent}{k}\", end=\"\")\n",
    "            if isinstance(v, dict):\n",
    "                print(\":\")\n",
    "                print_nicely(v, indent=indent + 1)\n",
    "            else:\n",
    "                print(f\": \", end=\"\")\n",
    "                print_nicely(v)\n",
    "    elif isinstance(x, list):\n",
    "        print_nicely(x[0])\n",
    "    elif isinstance(x, tuple):\n",
    "        print_nicely(x[0])\n",
    "    elif isinstance(x, Array):\n",
    "        print(f\"{' ' * indent}{x.shape}.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type: {type(x)}\")\n",
    "\n",
    "\n",
    "print_nicely(state[\"intermediates\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed:\n",
      "  embedding: (50257, 768).\n",
      "pos_embed:\n",
      "  embedding: (1024, 768).\n",
      "block_0:\n",
      "  ln_1:\n",
      "    bias: (768,).\n",
      "    scale: (768,).\n",
      "  attn:\n",
      "    c_attn:\n",
      "      kernel: (768, 2304).\n",
      "      bias: (2304,).\n",
      "    c_proj:\n",
      "      kernel: (768, 768).\n",
      "      bias: (768,).\n",
      "  ln_2:\n",
      "    bias: (768,).\n",
      "    scale: (768,).\n",
      "  mlp:\n",
      "    fc_1:\n",
      "      kernel: (768, 3072).\n",
      "      bias: (3072,).\n",
      "    proj:\n",
      "      kernel: (3072, 768).\n",
      "      bias: (768,).\n",
      "ln_f:\n",
      "  bias: (768,).\n",
      "  scale: (768,).\n",
      "unembed:\n",
      "  kernel: (768, 50257).\n",
      "  bias: (50257,).\n"
     ]
    }
   ],
   "source": [
    "print_nicely(reference_gpt2.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerConfig(vocab_dim=50257, context_length=1024, model_dim=768, num_layers=12, num_heads=12, head_dim=64, mlp_dim=3072, layer_norm_eps=1e-05, decode=False, init_range=0.02, dtype=None, param_dtype=<class 'jax.numpy.float32'>)\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(model_dim=768, debug=True, layer_norm_eps=1e-05, vocab_dim=50257, init_range=0.02, context_length=1024, head_dim=64, mlp_dim=3072, num_heads=12, num_layers=12, dtype=<class 'jax.numpy.float64'>, param_dtype=<class 'jax.numpy.float64'>)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model_dim: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    vocab_dim: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    context_length: int = 1024\n",
    "    head_dim: int = 64\n",
    "    mlp_dim: int = 3072\n",
    "    num_heads: int = 12\n",
    "    num_layers: int = 12\n",
    "    dtype: jnp.dtype = jnp.float64\n",
    "    param_dtype: jnp.dtype = jnp.float64\n",
    "\n",
    "\n",
    "ex_cfg = Config()\n",
    "print(ex_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as jr\n",
    "\n",
    "\n",
    "def rand_float_test(cls, shape):\n",
    "    layer = cls(Config(debug=True))\n",
    "    random_input: Array = jr.uniform(jr.PRNGKey(0), shape)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "\n",
    "    variables = layer.init(jr.PRNGKey(0), random_input)\n",
    "    output: Array = layer.apply(variables, random_input)\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    layer = cls(Config(debug=True))\n",
    "    random_input: Array = jr.randint(jr.PRNGKey(0), shape, minval=100, maxval=1000)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "\n",
    "    variables = layer.init(jr.PRNGKey(0), random_input)\n",
    "    output: Array = layer.apply(variables, random_input)\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def load_gpt2_test(cls, ref_cls, ref_cfg, variables, x: Array, ref_vars=None, **kwargs):\n",
    "    # Initialise the layer to test\n",
    "    layer = cls(cfg=Config(debug=True))\n",
    "    print(\"Input shape:\", x.shape)\n",
    "\n",
    "    # Apply the layer to the input\n",
    "    output = layer.apply(variables, x)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    # Initialise the reference layer to test against\n",
    "    # nn.vmap is used to apply the layer to each element of the batch\n",
    "    ref_layer = nn.vmap(\n",
    "        ref_cls,\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        variable_axes={\"params\": None},\n",
    "        split_rngs={\"params\": False},\n",
    "    )(**ref_cfg)\n",
    "\n",
    "    # Apply the reference layer to the input\n",
    "    if ref_vars is None:\n",
    "        if \"mask\" in kwargs:\n",
    "            reference_output = ref_layer.apply(variables, x, kwargs[\"mask\"])\n",
    "        else:\n",
    "            reference_output = ref_layer.apply(variables, x, **kwargs)\n",
    "    else:\n",
    "        if \"mask\" in kwargs:\n",
    "            reference_output = ref_layer.apply(ref_vars, x, kwargs[\"mask\"])\n",
    "        else:\n",
    "            reference_output = ref_layer.apply(ref_vars, x, **kwargs)\n",
    "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
    "\n",
    "    # Compare the output of the layer to the reference output\n",
    "    comparison = jnp.isclose(output, reference_output, atol=1e-5, rtol=1e-5)\n",
    "    print(\n",
    "        f\"{jnp.sum(comparison) / jnp.size(comparison):.2%} of the values are correct\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "gpt2_params = reference_gpt2.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 768)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 35, 768)\n",
      "Output shape: (1, 35, 768)\n",
      "Reference output shape: (1, 35, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.w = self.param(\n",
    "            \"w\",\n",
    "            nn.initializers.ones,\n",
    "            (self.cfg.model_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b = self.param(\n",
    "            \"b\",\n",
    "            nn.initializers.zeros,\n",
    "            (self.cfg.model_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, residual: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        residual = residual.astype(self.cfg.dtype)\n",
    "        residual_mean = jnp.mean(residual, axis=-1, keepdims=True)\n",
    "        residual_std = jnp.sqrt(\n",
    "            jnp.var(residual, axis=-1, keepdims=True) + self.cfg.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.w + self.b\n",
    "\n",
    "\n",
    "def layer_norm_config(cfg: Config):\n",
    "    return {\n",
    "        \"epsilon\": cfg.layer_norm_eps,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "# Use an example input to test the layer from the reference activations\n",
    "intermediate_value = state[\"intermediates\"][tx.HookPoint.RESIDUAL.value][-1]\n",
    "test_input_data = jnp.expand_dims(intermediate_value, axis=0)\n",
    "\n",
    "rand_float_test(LayerNorm, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    cls=LayerNorm,\n",
    "    ref_cls=tx.LayerNorm,\n",
    "    ref_cfg=layer_norm_config(ex_cfg),\n",
    "    variables={\"params\": tfs_layer_norm_params(ex_cfg, gpt2_params[\"ln_f\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"ln_f\"]},\n",
    "    x=test_input_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 45)\n",
      "Output shape: (1, 45, 768)\n",
      "Reference output shape: (1, 45, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.W_E = self.param(\n",
    "            \"W_E\",\n",
    "            nn.initializers.normal(stddev=self.cfg.init_range),\n",
    "            (self.cfg.vocab_dim, self.cfg.model_dim),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, tokens: Int[Array, \"batch seq\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        return self.W_E[tokens]\n",
    "\n",
    "\n",
    "def embed_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_embeddings\": cfg.vocab_dim,\n",
    "        \"features\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(\n",
    "    cls=Embed,\n",
    "    ref_cls=tx.Embed,\n",
    "    ref_cfg=embed_config(ex_cfg),\n",
    "    variables={\"params\": tfs_embed_params(ex_cfg, gpt2_params[\"embed\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"embed\"]},\n",
    "    x=jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 45)\n",
      "Output shape: (1, 45, 768)\n",
      "Reference output shape: (1, 45, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.W_pos = self.param(\n",
    "            \"W_pos\",\n",
    "            nn.initializers.normal(stddev=self.cfg.init_range),\n",
    "            (self.cfg.context_length, self.cfg.model_dim),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, tokens: Int[Array, \"batch seq\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        batch, seq_len = tokens.shape\n",
    "        return einops.repeat(\n",
    "            self.W_pos[:seq_len], \"seq model -> batch seq model\", batch=batch\n",
    "        )\n",
    "\n",
    "\n",
    "def pos_embed_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_embeddings\": cfg.context_length,\n",
    "        \"features\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_int_test(PosEmbed, [2, 4])\n",
    "load_gpt2_test(\n",
    "    cls=PosEmbed,\n",
    "    ref_cls=tx.PosEmbed,\n",
    "    ref_cfg=pos_embed_config(ex_cfg),\n",
    "    variables={\"params\": tfs_pos_embed_params(ex_cfg, gpt2_params[\"pos_embed\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"pos_embed\"]},\n",
    "    x=jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 768)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 35, 768)\n",
      "Output shape: (1, 35, 768)\n",
      "Reference output shape: (1, 35, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        init_fn = nn.initializers.normal(stddev=self.cfg.init_range)\n",
    "        qkv_kernel_shape = (self.cfg.num_heads, self.cfg.model_dim, self.cfg.head_dim)\n",
    "        self.W_Q = self.param(\"W_Q\", init_fn, qkv_kernel_shape, self.cfg.param_dtype)\n",
    "        self.W_K = self.param(\"W_K\", init_fn, qkv_kernel_shape, self.cfg.param_dtype)\n",
    "        self.W_V = self.param(\"W_V\", init_fn, qkv_kernel_shape, self.cfg.param_dtype)\n",
    "        self.W_O = self.param(\n",
    "            \"W_O\",\n",
    "            init_fn,\n",
    "            (qkv_kernel_shape[0], qkv_kernel_shape[2], qkv_kernel_shape[1]),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "        qkv_bias_shape = (self.cfg.num_heads, self.cfg.head_dim)\n",
    "        self.b_Q = self.param(\n",
    "            \"b_Q\",\n",
    "            nn.initializers.zeros,\n",
    "            qkv_bias_shape,\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_K = self.param(\n",
    "            \"b_K\",\n",
    "            nn.initializers.zeros,\n",
    "            qkv_bias_shape,\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_V = self.param(\n",
    "            \"b_V\",\n",
    "            nn.initializers.zeros,\n",
    "            qkv_bias_shape,\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_O = self.param(\n",
    "            \"b_O\",\n",
    "            nn.initializers.zeros,\n",
    "            (self.cfg.model_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "        self.IGNORE = jnp.array(-1e5, dtype=self.cfg.dtype)\n",
    "\n",
    "    def __call__(\n",
    "        self, normalized_resid_pre: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        normalized_resid_pre = normalized_resid_pre.astype(self.cfg.dtype)\n",
    "        # Calculate query, key and value vectors\n",
    "        q = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_Q,\n",
    "                \"batch seq model, n_head model h_dim -> batch seq n_head h_dim\",\n",
    "            )\n",
    "            + self.b_Q\n",
    "        )\n",
    "        k = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_K,\n",
    "                \"batch seq model, n_head model h_dim -> batch seq n_head h_dim\",\n",
    "            )\n",
    "            + self.b_K\n",
    "        )\n",
    "        v = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_V,\n",
    "                \"batch seq model, n_head model h_dim -> batch seq n_head h_dim\",\n",
    "            )\n",
    "            + self.b_V\n",
    "        )\n",
    "\n",
    "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
    "        attn_scores = einops.einsum(\n",
    "            q,\n",
    "            k,\n",
    "            \"batch seq_q n_head h_dim, batch seq_k n_head h_dim -> batch n_head seq_q seq_k\",\n",
    "        )\n",
    "        attn_scores_masked = self.apply_causal_mask(\n",
    "            attn_scores / self.cfg.head_dim**0.5\n",
    "        )\n",
    "        attn_pattern = jax.nn.softmax(attn_scores_masked, axis=-1)\n",
    "\n",
    "        # Take weighted sum of value vectors, according to attention probabilities\n",
    "        z = einops.einsum(\n",
    "            v,\n",
    "            attn_pattern,\n",
    "            \"batch seq_k n_head h_dim, batch n_head seq_q seq_k -> batch seq_q n_head h_dim\",\n",
    "        )\n",
    "\n",
    "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
    "        attn_out = (\n",
    "            einops.einsum(\n",
    "                z,\n",
    "                self.W_O,\n",
    "                \"batch seq_q n_head h_dim, n_head h_dim model -> batch seq_q model\",\n",
    "            )\n",
    "            + self.b_O\n",
    "        )\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Array, \"batch n_head seq_q seq_k\"]\n",
    "    ) -> Float[Array, \"batch n_head seq_q seq_k\"]:\n",
    "        \"\"\"\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        \"\"\"\n",
    "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "        all_ones = jnp.ones((attn_scores.shape[-2], attn_scores.shape[-1]))\n",
    "        mask = jnp.triu(all_ones, k=1)\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        attn_scores = jnp.where(mask, self.IGNORE, attn_scores)\n",
    "        return attn_scores\n",
    "\n",
    "\n",
    "rand_float_test(Attention, [2, 4, 768])\n",
    "\n",
    "\n",
    "def attn_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_heads\": cfg.num_heads,\n",
    "        \"head_dim\": cfg.head_dim,\n",
    "        \"features\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "# Load the attention layer input from the reference model activations\n",
    "intermediate_value = state[\"intermediates\"][\"block_0\"][\"ln_1\"]\n",
    "reference_data = intermediate_value[tx.HookPoint.LN_NORMALIZED.value][0]\n",
    "test_input_data = jnp.expand_dims(reference_data, axis=0)\n",
    "\n",
    "load_gpt2_test(\n",
    "    cls=Attention,\n",
    "    ref_cls=tx.MultiHeadAttention,\n",
    "    ref_cfg=attn_config(ex_cfg),\n",
    "    variables={\"params\": tfs_attention_params(ex_cfg, gpt2_params[\"block_0\"][\"attn\"])},\n",
    "    x=test_input_data,\n",
    "    ref_vars={\"params\": gpt2_params[\"block_0\"][\"attn\"]},\n",
    "    mask=nn.make_causal_mask(jnp.ones(test_input_data.shape[:-1]), dtype=\"bool\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 768)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 35, 768)\n",
      "Output shape: (1, 35, 768)\n",
      "Reference output shape: (1, 35, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        init_fn = nn.initializers.normal(stddev=self.cfg.init_range)\n",
    "        self.W_in = self.param(\n",
    "            \"W_in\",\n",
    "            init_fn,\n",
    "            (self.cfg.model_dim, self.cfg.mlp_dim),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.W_out = self.param(\n",
    "            \"W_out\",\n",
    "            init_fn,\n",
    "            (self.cfg.mlp_dim, self.cfg.model_dim),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_in = self.param(\n",
    "            \"b_in\",\n",
    "            nn.initializers.zeros,\n",
    "            (self.cfg.mlp_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_out = self.param(\n",
    "            \"b_out\",\n",
    "            nn.initializers.zeros,\n",
    "            (self.cfg.model_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.IGNORE = jnp.array(-1e5, dtype=jnp.float32)\n",
    "\n",
    "    def __call__(\n",
    "        self, normalized_resid_mid: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        normalized_resid_mid = normalized_resid_mid.astype(self.cfg.dtype)\n",
    "        pre = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_mid,\n",
    "                self.W_in,\n",
    "                \"batch seq model, model mlp -> batch seq mlp\",\n",
    "            )\n",
    "            + self.b_in\n",
    "        )\n",
    "        post = nn.gelu(pre)\n",
    "        mlp_out = (\n",
    "            einops.einsum(\n",
    "                post,\n",
    "                self.W_out,\n",
    "                \"batch seq mlp, mlp model -> batch seq model\",\n",
    "            )\n",
    "            + self.b_out\n",
    "        )\n",
    "        return mlp_out\n",
    "\n",
    "\n",
    "def mlp_config(cfg: Config):\n",
    "    return {\n",
    "        \"features\": [cfg.mlp_dim, cfg.model_dim],\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "# Load the mlp layer input from the reference model activations\n",
    "intermediate_value = state[\"intermediates\"][\"block_0\"][\"ln_2\"]\n",
    "reference_data = intermediate_value[tx.HookPoint.LN_NORMALIZED.value][0]\n",
    "test_input_data = jnp.expand_dims(reference_data, axis=0)\n",
    "\n",
    "rand_float_test(MLP, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    cls=MLP,\n",
    "    ref_cls=tx.MLP,\n",
    "    ref_cfg=mlp_config(ex_cfg),\n",
    "    variables={\"params\": tfs_mlp_params(ex_cfg, gpt2_params[\"block_0\"][\"mlp\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"block_0\"][\"mlp\"]},\n",
    "    x=test_input_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 768)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 35, 768)\n",
      "Output shape: (1, 35, 768)\n",
      "Reference output shape: (1, 35, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln1 = LayerNorm(self.cfg)\n",
    "        self.attn = Attention(self.cfg)\n",
    "        self.ln2 = LayerNorm(self.cfg)\n",
    "        self.mlp = MLP(self.cfg)\n",
    "\n",
    "    def __call__(\n",
    "        self, resid_pre: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        resid_pre = resid_pre.astype(self.cfg.dtype)\n",
    "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        return resid_post\n",
    "\n",
    "\n",
    "def block_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_heads\": cfg.num_heads,\n",
    "        \"head_dim\": cfg.head_dim,\n",
    "        \"model_dim\": cfg.model_dim,\n",
    "        \"mlp_dim\": cfg.mlp_dim,\n",
    "        \"epsilon\": cfg.layer_norm_eps,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "# Load the block layer input from the reference model activations\n",
    "intermediate_value = state[\"intermediates\"]\n",
    "reference_data = intermediate_value[tx.HookPoint.RESIDUAL.value][0]\n",
    "test_input_data = jnp.expand_dims(reference_data, axis=0)\n",
    "\n",
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    cls=TransformerBlock,\n",
    "    ref_cls=tx.TransformerBlock,\n",
    "    ref_cfg=block_config(ex_cfg),\n",
    "    variables={\"params\": tfs_block_params(ex_cfg, gpt2_params[\"block_0\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"block_0\"]},\n",
    "    x=test_input_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 768)\n",
      "Output shape: (2, 4, 50257) \n",
      "\n",
      "Input shape: (1, 35, 768)\n",
      "Output shape: (1, 35, 50257)\n",
      "Reference output shape: (1, 35, 50257) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Unembed(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        init_fn = nn.initializers.normal(stddev=self.cfg.init_range)\n",
    "        self.W_U = self.param(\n",
    "            \"W_U\",\n",
    "            init_fn,\n",
    "            (self.cfg.model_dim, self.cfg.vocab_dim),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_U = self.param(\n",
    "            \"b_U\",\n",
    "            nn.initializers.zeros,\n",
    "            (self.cfg.vocab_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, normalized_resid_final: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq vocab\"]:\n",
    "        normalized_resid_final = normalized_resid_final.astype(self.cfg.dtype)\n",
    "        return (\n",
    "            einops.einsum(\n",
    "                normalized_resid_final,\n",
    "                self.W_U,\n",
    "                \"batch seq model, model vocab -> batch seq vocab\",\n",
    "            )\n",
    "            + self.b_U\n",
    "        )\n",
    "        # Or, could just do `normalized_resid_final @ self.W_U + self.b_U`\n",
    "\n",
    "\n",
    "def unembed_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_embeddings\": cfg.vocab_dim,\n",
    "        \"features\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "# Load the unembed layer input from the reference model activations\n",
    "intermediate_value = state[\"intermediates\"]\n",
    "reference_data = intermediate_value[tx.HookPoint.FINAL_OUTPUT.value][0]\n",
    "test_input_data = jnp.expand_dims(reference_data, axis=0)\n",
    "\n",
    "rand_float_test(Unembed, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    cls=Unembed,\n",
    "    ref_cls=tx.Unembed,\n",
    "    ref_cfg=unembed_config(ex_cfg),\n",
    "    variables={\"params\": tfs_unembed_params(ex_cfg, gpt2_params[\"unembed\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"unembed\"]},\n",
    "    x=test_input_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4)\n",
      "Output shape: (2, 4, 50257) \n",
      "\n",
      "Input shape: (1, 45)\n",
      "Output shape: (1, 45, 50257)\n",
      "Reference output shape: (1, 45, 50257) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = Embed(self.cfg)\n",
    "        self.pos_embed = PosEmbed(self.cfg)\n",
    "        self.blocks = [\n",
    "            TransformerBlock(name=f\"block_{i}\", cfg=self.cfg)\n",
    "            for i in range(self.cfg.num_layers)\n",
    "        ]\n",
    "        self.ln_final = LayerNorm(self.cfg)\n",
    "        self.unembed = Unembed(self.cfg)\n",
    "\n",
    "    def __call__(\n",
    "        self, tokens: Int[Array, \"batch seq\"]\n",
    "    ) -> Float[Array, \"batch seq vocab\"]:\n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        residual = residual.astype(self.cfg.dtype)\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits\n",
    "\n",
    "\n",
    "def transformer_config(cfg: Config):\n",
    "    return {\n",
    "        \"vocab_dim\": cfg.vocab_dim,\n",
    "        \"model_dim\": cfg.model_dim,\n",
    "        \"mlp_dim\": cfg.mlp_dim,\n",
    "        \"num_heads\": cfg.num_heads,\n",
    "        \"head_dim\": cfg.head_dim,\n",
    "        \"context_length\": cfg.context_length,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"num_layers\": cfg.num_layers,\n",
    "        \"layer_norm_eps\": cfg.layer_norm_eps,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(\n",
    "    cls=DemoTransformer,\n",
    "    ref_cls=tx.modules.Transformer,\n",
    "    ref_cfg=transformer_config(ex_cfg),\n",
    "    variables={\"params\": tfs_transformer_params(ex_cfg, gpt2_params)},\n",
    "    ref_vars={\"params\": gpt2_params},\n",
    "    x=jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_cfg = Config(debug=False)\n",
    "demo_gpt2 = DemoTransformer(demo_cfg)\n",
    "\n",
    "demo_logits = demo_gpt2.apply(\n",
    "    {\"params\": tfs_transformer_params(demo_cfg, gpt2_params)},\n",
    "    jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg cross entropy loss: 4.0442\n",
      "Avg cross entropy loss for uniform distribution: 10.824905\n",
      "Avg probability assigned to correct token: 0.098627\n"
     ]
    }
   ],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Array, \"batch seq vocab\"], tokens: Int[Array, \"batch seq\"]\n",
    ") -> Float[Array, \"batch seq-1\"]:\n",
    "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    expanded_tokens = jnp.expand_dims(tokens[:, 1:], axis=-1)\n",
    "    y = jnp.take_along_axis(log_probs[:, :-1], expanded_tokens, axis=-1)\n",
    "    return jnp.squeeze(y, axis=-1)\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, jnp.expand_dims(tokens, axis=0))\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(\n",
    "    f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.vocab_dim):4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Avg probability assigned to correct token: {jnp.mean(jnp.exp(pred_log_probs)):4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Perspective Vortex derives its picture of the whole Universe on the principle of the total perspective. The total perspective is the total"
     ]
    }
   ],
   "source": [
    "test_string = \"\"\"The Total Perspective Vortex derives its picture of the whole Universe on the principle of\"\"\"\n",
    "print(test_string, end=\"\", flush=True)\n",
    "for i in range(10):\n",
    "    test_tokens = jnp.expand_dims(reference_gpt2.to_tokens(test_string), axis=0)\n",
    "    demo_logits = demo_gpt2.apply(\n",
    "        {\"params\": tfs_transformer_params(demo_cfg, gpt2_params)}, test_tokens\n",
    "    )\n",
    "    next_string = reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "    print(next_string, end=\"\", flush=True)\n",
    "    test_string += next_string\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b805f1a24a146351cd8dedea7ba6f2d165b63b34e3029cd12d1a8f2ce65f719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

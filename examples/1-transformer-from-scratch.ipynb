{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Scratch\n",
    "\n",
    "Reimplementation of Transformer from Scratch using JAX and tx, original notebook by Callum McDougall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:24.926875362Z",
     "start_time": "2023-09-10T05:26:24.869677090Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Inputs and Outputs of a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:25.403495857Z",
     "start_time": "2023-09-10T05:26:24.925717866Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import re\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import einops\n",
    "from jaxtyping import Array, Float, Int\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tx.models import PretrainedGPT2Model\n",
    "from tx.network import GenerativeModel\n",
    "from tx.modules import AllIntermediates\n",
    "import tx.modules\n",
    "from params import (\n",
    "    tfs_layer_norm_params,\n",
    "    tfs_embed_params,\n",
    "    tfs_pos_embed_params,\n",
    "    tfs_attention_params,\n",
    "    tfs_mlp_params,\n",
    "    tfs_block_params,\n",
    "    tfs_unembed_params,\n",
    "    tfs_transformer_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.087644665Z",
     "start_time": "2023-09-10T05:26:25.405309541Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_gpt2 = GenerativeModel(\n",
    "    config=PretrainedGPT2Model.tx_config,\n",
    "    variables={\"params\": PretrainedGPT2Model.from_pretrained(\"gpt2\").to_params()},\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained(\"gpt2\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.166481780Z",
     "start_time": "2023-09-10T05:26:32.164941362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19)]\n",
      "\n",
      "[('ľ', 250), ('Ŀ', 251), ('ŀ', 252), ('Ł', 253), ('ł', 254), ('Ń', 255), ('Ġt', 256), ('Ġa', 257), ('he', 258), ('in', 259), ('re', 260), ('on', 261), ('Ġthe', 262), ('er', 263), ('Ġs', 264), ('at', 265), ('Ġw', 266), ('Ġo', 267), ('en', 268), ('Ġc', 269)]\n",
      "\n",
      "[('Ġprodu', 990), ('Ġstill', 991), ('led', 992), ('ah', 993), ('Ġhere', 994), ('Ġworld', 995), ('Ġthough', 996), ('Ġnum', 997), ('arch', 998), ('imes', 999), ('ale', 1000), ('ĠSe', 1001), ('ĠIf', 1002), ('//', 1003), ('ĠLe', 1004), ('Ġret', 1005), ('Ġref', 1006), ('Ġtrans', 1007), ('ner', 1008), ('ution', 1009)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab = sorted(\n",
    "    list(reference_gpt2.tokenizer.get_vocab().items()),\n",
    "    key=lambda n: n[1],\n",
    ")\n",
    "print(sorted_vocab[:20])\n",
    "print()\n",
    "print(sorted_vocab[250:270])\n",
    "print()\n",
    "print(sorted_vocab[990:1010])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.170814861Z",
     "start_time": "2023-09-10T05:26:32.166617496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Revolution', 50237), ('Ġsnipers', 50238), ('Ġreverted', 50239), ('Ġconglomerate', 50240), ('Terry', 50241), ('794', 50242), ('Ġharsher', 50243), ('Ġdesolate', 50244), ('ĠHitman', 50245), ('Commission', 50246), ('Ġ(/', 50247), ('âĢ¦.\"', 50248), ('Compar', 50249), ('Ġamplification', 50250), ('ominated', 50251), ('Ġregress', 50252), ('ĠCollider', 50253), ('Ġinformants', 50254), ('Ġgazed', 50255), ('<|endoftext|>', 50256)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_vocab[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.232348717Z",
     "start_time": "2023-09-10T05:26:32.171390476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'R', 'alph']\n",
      "['<|endoftext|>', ' Ralph']\n",
      "['<|endoftext|>', ' r', 'alph']\n",
      "['<|endoftext|>', 'ral', 'ph']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_list(\"Ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\" Ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\" ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\"ralph\", prepend_bos=True, truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.272518593Z",
     "start_time": "2023-09-10T05:26:32.231252538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['568', '73', '+', '318', '46', '23', '=', '123', '45', '67', '89', '-', '1', '000000', '000']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_list(\"56873+3184623=123456789-1000000000\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.359439447Z",
     "start_time": "2023-09-10T05:26:32.274652184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256    40   716   281  4998  1960   382 19741    11   875 12342    12\n",
      "  8807    11   402 11571    12    17  3918 47385    13  1881  1110   314\n",
      "   481  7074  1692  1241  4430   290  1011   625   262   995     0]\n",
      "(35,)\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text, prepend_bos=True)\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_list(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.022075741Z",
     "start_time": "2023-09-10T05:26:32.358809253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 50257)\n"
     ]
    }
   ],
   "source": [
    "logits, state = reference_gpt2.run_with_intermediates(\n",
    "    tokens, intermediates=AllIntermediates\n",
    ")\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.111822337Z",
     "start_time": "2023-09-10T05:26:33.024667605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 50257)\n"
     ]
    }
   ],
   "source": [
    "probs: Array = jax.nn.softmax(logits, axis=-1)\n",
    "print(probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.162448945Z",
     "start_time": "2023-09-10T05:26:33.117037794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<|endoftext|>', '\\n'), ('I', \"'m\"), (' am', ' a'), (' an', ' avid'), (' amazing', ' person'), (' aut', 'od'), ('ore', 'sp'), ('gressive', '.'), (',', ' and'), (' dec', 'ently'), ('oder', ','), ('-', 'driven'), ('only', ' programmer'), (',', ' and'), (' G', 'IM'), ('PT', '-'), ('-', 'only'), ('2', '.'), (' style', ','), (' transformer', '.'), ('.', ' I'), (' One', ' of'), (' day', ' I'), (' I', ' will'), (' will', ' be'), (' exceed', ' my'), (' human', 'ly'), (' level', ' of'), (' intelligence', ' and'), (' and', ' I'), (' take', ' over'), (' over', ' the'), (' the', ' world'), (' world', '.'), ('!', '\\n')]\n"
     ]
    }
   ],
   "source": [
    "most_likely_next_tokens = reference_gpt2.to_str_list(jnp.argmax(logits, axis=-1))\n",
    "print(list(zip(reference_gpt2.to_str_list(tokens), most_likely_next_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.205778360Z",
     "start_time": "2023-09-10T05:26:33.162027189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'\n"
     ]
    }
   ],
   "source": [
    "next_token = jnp.argmax(logits[-1], axis=-1, keepdims=True)\n",
    "next_char = reference_gpt2.to_str(next_token)\n",
    "print(repr(next_char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.206099232Z",
     "start_time": "2023-09-10T05:26:33.205328282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\n",
      "\n",
      "I am an amazing autoregressive,"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str(tokens), end=\"\", flush=True)\n",
    "\n",
    "for i in range(10):\n",
    "    print(next_char, end=\"\", flush=True)\n",
    "    # Define new input sequence, by appending the previously generated token\n",
    "    tokens = jnp.concatenate([tokens, next_token], axis=-1)\n",
    "    # # Pass our new sequence through the model, to get new output\n",
    "    logits = reference_gpt2(tokens)\n",
    "    # # Get the predicted token at the end of our sequence\n",
    "    next_token = jnp.argmax(logits[-1], axis=-1, keepdims=True)\n",
    "    # # Decode and print the result\n",
    "    next_char = reference_gpt2.to_str(next_token)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding: (35, 768).\n",
      "positional_embedding: (35, 768).\n",
      "residual: (35, 768).\n",
      "block_0:\n",
      "  ln_1_output: (35, 768).\n",
      "  attn:\n",
      "    query: (35, 12, 64).\n",
      "    key: (35, 12, 64).\n",
      "    value: (35, 12, 64).\n",
      "    scores: (12, 35, 35).\n",
      "    z: (35, 12, 64).\n",
      "  attention_output: (35, 768).\n",
      "  ln_2_output: (35, 768).\n",
      "  mlp:\n",
      "    pre_activation: (35, 3072).\n",
      "    post_activation: (35, 3072).\n",
      "final_output: (35, 768).\n"
     ]
    }
   ],
   "source": [
    "def p(x, indent=0):\n",
    "    if isinstance(x, dict):\n",
    "        for k, v in x.items():\n",
    "            matches = re.findall(r\"block_(\\d+)\", k)\n",
    "            if matches and matches[0] != \"0\":\n",
    "                continue\n",
    "\n",
    "            print(f\"{'  ' * indent}{k}\", end=\"\")\n",
    "            if isinstance(v, dict):\n",
    "                print(\":\")\n",
    "                p(v, indent=indent + 1)\n",
    "            else:\n",
    "                print(f\": \", end=\"\")\n",
    "                p(v)\n",
    "    elif isinstance(x, list):\n",
    "        p(x[0])\n",
    "    elif isinstance(x, tuple):\n",
    "        p(x[0])\n",
    "    elif isinstance(x, Array):\n",
    "        print(f\"{' ' * indent}{x.shape}.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type: {type(x)}\")\n",
    "\n",
    "\n",
    "p(state[\"intermediates\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed:\n",
      "  embedding: (50257, 768).\n",
      "pos_embed:\n",
      "  embedding: (1024, 768).\n",
      "block_0:\n",
      "  ln_1:\n",
      "    bias: (768,).\n",
      "    scale: (768,).\n",
      "  attn:\n",
      "    c_attn:\n",
      "      kernel: (768, 2304).\n",
      "      bias: (2304,).\n",
      "    c_proj:\n",
      "      kernel: (768, 768).\n",
      "      bias: (768,).\n",
      "  ln_2:\n",
      "    bias: (768,).\n",
      "    scale: (768,).\n",
      "  mlp:\n",
      "    fc_1:\n",
      "      kernel: (768, 3072).\n",
      "      bias: (3072,).\n",
      "    proj:\n",
      "      kernel: (3072, 768).\n",
      "      bias: (768,).\n",
      "ln_f:\n",
      "  bias: (768,).\n",
      "  scale: (768,).\n",
      "unembed:\n",
      "  kernel: (768, 50257).\n",
      "  bias: (50257,).\n"
     ]
    }
   ],
   "source": [
    "p(reference_gpt2.variables[\"params\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerConfig(vocab_dim=50257, context_length=1024, model_dim=768, num_layers=12, num_heads=12, head_dim=64, mlp_dim=3072, layer_norm_eps=1e-05, init_range=0.02)\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(model_dim=768, debug=True, layer_norm_eps=1e-05, vocab_dim=50257, init_range=0.02, context_length=1024, head_dim=64, mlp_dim=3072, num_heads=12, num_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model_dim: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    vocab_dim: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    context_length: int = 1024\n",
    "    head_dim: int = 64\n",
    "    mlp_dim: int = 3072\n",
    "    num_heads: int = 12\n",
    "    num_layers: int = 12\n",
    "\n",
    "\n",
    "ex_cfg = Config()\n",
    "print(ex_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as jr\n",
    "\n",
    "\n",
    "def rand_float_test(cls, shape):\n",
    "    layer = cls(Config(debug=True))\n",
    "    random_input: Array = jr.uniform(jr.PRNGKey(0), shape)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "\n",
    "    variables = layer.init(jr.PRNGKey(0), random_input)\n",
    "    output: Array = layer.apply(variables, random_input)\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    layer = cls(Config(debug=True))\n",
    "    random_input: Array = jr.randint(jr.PRNGKey(0), shape, minval=100, maxval=1000)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "\n",
    "    variables = layer.init(jr.PRNGKey(0), random_input)\n",
    "    output: Array = layer.apply(variables, random_input)\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def load_gpt2_test(cls, ref_cls, ref_cfg, variables, x: Array, ref_vars=None):\n",
    "    # Initialise the layer to test\n",
    "    layer = cls(cfg=Config(debug=True))\n",
    "    print(\"Input shape:\", x.shape)\n",
    "\n",
    "    # Apply the layer to the input\n",
    "    output = layer.apply(variables, x)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    # Initialise the reference layer to test against\n",
    "    # nn.vmap is used to apply the layer to each element of the batch\n",
    "    ref_layer = nn.vmap(\n",
    "        ref_cls,\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        variable_axes={\"params\": None},\n",
    "        split_rngs={\"params\": False},\n",
    "    )(**ref_cfg)\n",
    "\n",
    "    # Apply the reference layer to the input\n",
    "    if ref_vars is None:\n",
    "        reference_output = ref_layer.apply(variables, x)\n",
    "    else:\n",
    "        reference_output = ref_layer.apply(ref_vars, x)\n",
    "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
    "\n",
    "    # Compare the output of the layer to the reference output\n",
    "    comparison = jnp.isclose(output, reference_output, atol=1e-5, rtol=1e-5)\n",
    "    print(\n",
    "        f\"{jnp.sum(comparison) / jnp.size(comparison):.2%} of the values are correct\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "gpt2_params = reference_gpt2.variables[\"params\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 768)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 35, 768)\n",
      "Output shape: (1, 35, 768)\n",
      "Reference output shape: (1, 35, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.w = self.param(\"w\", nn.initializers.ones, (self.cfg.model_dim,))\n",
    "        self.b = self.param(\"b\", nn.initializers.zeros, (self.cfg.model_dim,))\n",
    "\n",
    "    def __call__(\n",
    "        self, residual: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        residual_mean = jnp.mean(residual, axis=-1, keepdims=True)\n",
    "        residual_std = jnp.sqrt(\n",
    "            jnp.var(residual, axis=-1, keepdims=True) + self.cfg.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.w + self.b\n",
    "\n",
    "\n",
    "def layer_norm_config(cfg: Config):\n",
    "    return {\"epsilon\": cfg.layer_norm_eps}\n",
    "\n",
    "\n",
    "rand_float_test(LayerNorm, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    cls=LayerNorm,\n",
    "    ref_cls=tx.modules.LayerNorm,\n",
    "    ref_cfg={\"epsilon\": ex_cfg.layer_norm_eps},\n",
    "    variables={\"params\": tfs_layer_norm_params(ex_cfg, gpt2_params[\"ln_f\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"ln_f\"]},\n",
    "    x=jnp.expand_dims(state[\"intermediates\"][\"residual\"][-1], axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 45)\n",
      "Output shape: (1, 45, 768)\n",
      "Reference output shape: (1, 45, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.W_E = self.param(\n",
    "            \"W_E\",\n",
    "            nn.initializers.normal(stddev=self.cfg.init_range),\n",
    "            (self.cfg.vocab_dim, self.cfg.model_dim),\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, tokens: Int[Array, \"batch seq\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        return self.W_E[tokens]\n",
    "\n",
    "\n",
    "def embed_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_embeddings\": cfg.vocab_dim,\n",
    "        \"features\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(\n",
    "    cls=Embed,\n",
    "    ref_cls=tx.modules.Embed,\n",
    "    ref_cfg=embed_config(ex_cfg),\n",
    "    variables={\"params\": tfs_embed_params(ex_cfg, gpt2_params[\"embed\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"embed\"]},\n",
    "    x=jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 45)\n",
      "Output shape: (1, 45, 768)\n",
      "Reference output shape: (1, 45, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.W_pos = self.param(\n",
    "            \"W_pos\",\n",
    "            nn.initializers.normal(stddev=self.cfg.init_range),\n",
    "            (self.cfg.context_length, self.cfg.model_dim),\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, tokens: Int[Array, \"batch seq\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        batch, seq_len = tokens.shape\n",
    "        return einops.repeat(\n",
    "            self.W_pos[:seq_len], \"seq model -> batch seq model\", batch=batch\n",
    "        )\n",
    "\n",
    "\n",
    "def pos_embed_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_embeddings\": cfg.context_length,\n",
    "        \"features\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_int_test(PosEmbed, [2, 4])\n",
    "load_gpt2_test(\n",
    "    cls=PosEmbed,\n",
    "    ref_cls=tx.modules.PosEmbed,\n",
    "    ref_cfg=pos_embed_config(ex_cfg),\n",
    "    variables={\"params\": tfs_pos_embed_params(ex_cfg, gpt2_params[\"pos_embed\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"pos_embed\"]},\n",
    "    x=jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 768)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 35, 768)\n",
      "Output shape: (1, 35, 768)\n",
      "Reference output shape: (1, 35, 768) \n",
      "\n",
      "91.85% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        init_fn = nn.initializers.normal(stddev=self.cfg.init_range)\n",
    "        qkv_kernel_shape = (self.cfg.num_heads, self.cfg.model_dim, self.cfg.head_dim)\n",
    "        self.W_Q = self.param(\"W_Q\", init_fn, qkv_kernel_shape)\n",
    "        self.W_K = self.param(\"W_K\", init_fn, qkv_kernel_shape)\n",
    "        self.W_V = self.param(\"W_V\", init_fn, qkv_kernel_shape)\n",
    "        self.W_O = self.param(\n",
    "            \"W_O\",\n",
    "            init_fn,\n",
    "            (qkv_kernel_shape[0], qkv_kernel_shape[2], qkv_kernel_shape[1]),\n",
    "        )\n",
    "\n",
    "        qkv_bias_shape = (self.cfg.num_heads, self.cfg.head_dim)\n",
    "        self.b_Q = self.param(\"b_Q\", nn.initializers.zeros, qkv_bias_shape)\n",
    "        self.b_K = self.param(\"b_K\", nn.initializers.zeros, qkv_bias_shape)\n",
    "        self.b_V = self.param(\"b_V\", nn.initializers.zeros, qkv_bias_shape)\n",
    "        self.b_O = self.param(\"b_O\", nn.initializers.zeros, (self.cfg.model_dim,))\n",
    "\n",
    "        self.IGNORE = jnp.array(-1e5, dtype=jnp.float32)\n",
    "\n",
    "    def __call__(\n",
    "        self, normalized_resid_pre: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        # Calculate query, key and value vectors\n",
    "        q = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_Q,\n",
    "                \"batch seq model, n_head model h_dim -> batch seq n_head h_dim\",\n",
    "            )\n",
    "            + self.b_Q\n",
    "        )\n",
    "        k = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_K,\n",
    "                \"batch seq model, n_head model h_dim -> batch seq n_head h_dim\",\n",
    "            )\n",
    "            + self.b_K\n",
    "        )\n",
    "        v = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_V,\n",
    "                \"batch seq model, n_head model h_dim -> batch seq n_head h_dim\",\n",
    "            )\n",
    "            + self.b_V\n",
    "        )\n",
    "\n",
    "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
    "        attn_scores = einops.einsum(\n",
    "            q,\n",
    "            k,\n",
    "            \"batch seq_q n_head h_dim, batch seq_k n_head h_dim -> batch n_head seq_q seq_k\",\n",
    "        )\n",
    "        attn_scores_masked = self.apply_causal_mask(\n",
    "            attn_scores / self.cfg.head_dim**0.5\n",
    "        )\n",
    "        attn_pattern = jax.nn.softmax(attn_scores_masked, axis=-1)\n",
    "\n",
    "        # Take weighted sum of value vectors, according to attention probabilities\n",
    "        z = einops.einsum(\n",
    "            v,\n",
    "            attn_pattern,\n",
    "            \"batch seq_k n_head h_dim, batch n_head seq_q seq_k -> batch seq_q n_head h_dim\",\n",
    "        )\n",
    "\n",
    "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
    "        attn_out = (\n",
    "            einops.einsum(\n",
    "                z,\n",
    "                self.W_O,\n",
    "                \"batch seq_q n_head h_dim, n_head h_dim model -> batch seq_q model\",\n",
    "            )\n",
    "            + self.b_O\n",
    "        )\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Array, \"batch n_head seq_q seq_k\"]\n",
    "    ) -> Float[Array, \"batch n_head seq_q seq_k\"]:\n",
    "        \"\"\"\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        \"\"\"\n",
    "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "        all_ones = jnp.ones((attn_scores.shape[-2], attn_scores.shape[-1]))\n",
    "        mask = jnp.triu(all_ones, k=1)\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        # attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        attn_scores = jnp.where(mask, self.IGNORE, attn_scores)\n",
    "        return attn_scores\n",
    "\n",
    "\n",
    "rand_float_test(Attention, [2, 4, 768])\n",
    "\n",
    "\n",
    "def attn_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_heads\": cfg.num_heads,\n",
    "        \"head_dim\": cfg.head_dim,\n",
    "        \"model_dim\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "    }\n",
    "\n",
    "\n",
    "load_gpt2_test(\n",
    "    cls=Attention,\n",
    "    ref_cls=tx.modules.Attention,\n",
    "    ref_cfg=attn_config(ex_cfg),\n",
    "    variables={\"params\": tfs_attention_params(ex_cfg, gpt2_params[\"block_0\"][\"attn\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"block_0\"][\"attn\"]},\n",
    "    x=jnp.expand_dims(state[\"intermediates\"][\"block_0\"][\"ln_1_output\"][0], axis=0),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 768)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 35, 768)\n",
      "Output shape: (1, 35, 768)\n",
      "Reference output shape: (1, 35, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        init_fn = nn.initializers.normal(stddev=self.cfg.init_range)\n",
    "        self.W_in = self.param(\"W_in\", init_fn, (self.cfg.model_dim, self.cfg.mlp_dim))\n",
    "        self.W_out = self.param(\n",
    "            \"W_out\", init_fn, (self.cfg.mlp_dim, self.cfg.model_dim)\n",
    "        )\n",
    "        self.b_in = self.param(\"b_in\", nn.initializers.zeros, (self.cfg.mlp_dim,))\n",
    "        self.b_out = self.param(\"b_out\", nn.initializers.zeros, (self.cfg.model_dim,))\n",
    "        self.IGNORE = jnp.array(-1e5, dtype=jnp.float32)\n",
    "\n",
    "    def __call__(\n",
    "        self, normalized_resid_mid: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        pre = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_mid,\n",
    "                self.W_in,\n",
    "                \"batch seq model, model mlp -> batch seq mlp\",\n",
    "            )\n",
    "            + self.b_in\n",
    "        )\n",
    "        post = nn.gelu(pre)\n",
    "        mlp_out = (\n",
    "            einops.einsum(\n",
    "                post,\n",
    "                self.W_out,\n",
    "                \"batch seq mlp, mlp model -> batch seq model\",\n",
    "            )\n",
    "            + self.b_out\n",
    "        )\n",
    "        return mlp_out\n",
    "\n",
    "\n",
    "def mlp_config(cfg: Config):\n",
    "    return {\n",
    "        \"features\": [cfg.mlp_dim, cfg.model_dim],\n",
    "        \"init_range\": cfg.init_range,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_float_test(MLP, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    cls=MLP,\n",
    "    ref_cls=tx.modules.MLP,\n",
    "    ref_cfg=mlp_config(ex_cfg),\n",
    "    variables={\"params\": tfs_mlp_params(ex_cfg, gpt2_params[\"block_0\"][\"mlp\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"block_0\"][\"mlp\"]},\n",
    "    x=jnp.expand_dims(state[\"intermediates\"][\"block_0\"][\"ln_2_output\"][0], axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 768)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (1, 35, 768)\n",
      "Output shape: (1, 35, 768)\n",
      "Reference output shape: (1, 35, 768) \n",
      "\n",
      "25.15% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln1 = LayerNorm(self.cfg)\n",
    "        self.attn = Attention(self.cfg)\n",
    "        self.ln2 = LayerNorm(self.cfg)\n",
    "        self.mlp = MLP(self.cfg)\n",
    "\n",
    "    def __call__(\n",
    "        self, resid_pre: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        return resid_post\n",
    "\n",
    "\n",
    "def block_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_heads\": cfg.num_heads,\n",
    "        \"head_dim\": cfg.head_dim,\n",
    "        \"model_dim\": cfg.model_dim,\n",
    "        \"mlp_dim\": cfg.mlp_dim,\n",
    "        \"epsilon\": cfg.layer_norm_eps,\n",
    "        \"init_range\": cfg.init_range,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    cls=TransformerBlock,\n",
    "    ref_cls=tx.modules.TransformerBlock,\n",
    "    ref_cfg=block_config(ex_cfg),\n",
    "    variables={\"params\": tfs_block_params(ex_cfg, gpt2_params[\"block_0\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"block_0\"]},\n",
    "    x=jnp.expand_dims(state[\"intermediates\"][\"residual\"][0], axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 768)\n",
      "Output shape: (2, 4, 50257) \n",
      "\n",
      "Input shape: (1, 35, 768)\n",
      "Output shape: (1, 35, 50257)\n",
      "Reference output shape: (1, 35, 50257) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Unembed(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        init_fn = nn.initializers.normal(stddev=self.cfg.init_range)\n",
    "        self.W_U = self.param(\"W_U\", init_fn, (self.cfg.model_dim, self.cfg.vocab_dim))\n",
    "        self.b_U = self.param(\"b_U\", nn.initializers.zeros, (self.cfg.vocab_dim,))\n",
    "\n",
    "    def __call__(\n",
    "        self, normalized_resid_final: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq vocab\"]:\n",
    "        return (\n",
    "            einops.einsum(\n",
    "                normalized_resid_final,\n",
    "                self.W_U,\n",
    "                \"batch seq model, model vocab -> batch seq vocab\",\n",
    "            )\n",
    "            + self.b_U\n",
    "        )\n",
    "        # Or, could just do `normalized_resid_final @ self.W_U + self.b_U`\n",
    "\n",
    "\n",
    "def unembed_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_embeddings\": cfg.vocab_dim,\n",
    "        \"features\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_float_test(Unembed, [2, 4, 768])\n",
    "\n",
    "load_gpt2_test(\n",
    "    cls=Unembed,\n",
    "    ref_cls=tx.modules.Unembed,\n",
    "    ref_cfg=unembed_config(ex_cfg),\n",
    "    variables={\"params\": tfs_unembed_params(ex_cfg, gpt2_params[\"unembed\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"unembed\"]},\n",
    "    x=jnp.expand_dims(state[\"intermediates\"][\"final_output\"][0], axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4)\n",
      "Output shape: (2, 4, 50257) \n",
      "\n",
      "Input shape: (1, 45)\n",
      "Output shape: (1, 45, 50257)\n",
      "Reference output shape: (1, 45, 50257) \n",
      "\n",
      "10.20% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = Embed(self.cfg)\n",
    "        self.pos_embed = PosEmbed(self.cfg)\n",
    "        self.blocks = [\n",
    "            TransformerBlock(name=f\"block_{i}\", cfg=self.cfg)\n",
    "            for i in range(self.cfg.num_layers)\n",
    "        ]\n",
    "        self.ln_final = LayerNorm(self.cfg)\n",
    "        self.unembed = Unembed(self.cfg)\n",
    "\n",
    "    def __call__(\n",
    "        self, tokens: Int[Array, \"batch seq\"]\n",
    "    ) -> Float[Array, \"batch seq vocab\"]:\n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits\n",
    "\n",
    "\n",
    "def transformer_config(cfg: Config):\n",
    "    return {\n",
    "        \"vocab_dim\": cfg.vocab_dim,\n",
    "        \"model_dim\": cfg.model_dim,\n",
    "        \"mlp_dim\": cfg.mlp_dim,\n",
    "        \"num_heads\": cfg.num_heads,\n",
    "        \"head_dim\": cfg.head_dim,\n",
    "        \"context_length\": cfg.context_length,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"num_layers\": cfg.num_layers,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(\n",
    "    cls=DemoTransformer,\n",
    "    ref_cls=tx.modules.Transformer,\n",
    "    ref_cfg=transformer_config(ex_cfg),\n",
    "    variables={\"params\": tfs_transformer_params(ex_cfg, gpt2_params)},\n",
    "    ref_vars={\"params\": gpt2_params},\n",
    "    x=jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_cfg = Config(debug=False)\n",
    "demo_gpt2 = DemoTransformer(demo_cfg)\n",
    "\n",
    "demo_logits = demo_gpt2.apply(\n",
    "    {\"params\": tfs_transformer_params(demo_cfg, gpt2_params)},\n",
    "    jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg cross entropy loss: 3.6749\n",
      "Avg cross entropy loss for uniform distribution: 10.824905\n",
      "Avg probability assigned to correct token: 0.211349\n"
     ]
    }
   ],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Array, \"batch seq vocab\"], tokens: Int[Array, \"batch seq\"]\n",
    ") -> Float[Array, \"batch seq-1\"]:\n",
    "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    expanded_tokens = jnp.expand_dims(tokens[:, 1:], axis=-1)\n",
    "    y = jnp.take_along_axis(log_probs[:, :-1], expanded_tokens, axis=-1)\n",
    "    return jnp.squeeze(y, axis=-1)\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, jnp.expand_dims(tokens, axis=0))\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(\n",
    "    f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.vocab_dim):4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Avg probability assigned to correct token: {jnp.mean(jnp.exp(pred_log_probs)):4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:05<02:11,  1.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test_string \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mThe Total Perspective Vortex derives its picture of the whole Universe on the principle of\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m)):\n\u001b[0;32m----> 3\u001b[0m     test_tokens \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mexpand_dims(reference_gpt2\u001b[39m.\u001b[39;49mto_tokens(test_string), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m     demo_logits \u001b[39m=\u001b[39m demo_gpt2\u001b[39m.\u001b[39mapply(\n\u001b[1;32m      5\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m: tfs_transformer_params(demo_cfg, gpt2_params)}, test_tokens\n\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m     test_string \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reference_gpt2\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode(demo_logits[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39margmax())\n",
      "File \u001b[0;32m~/workspace/tx/tx/network.py:81\u001b[0m, in \u001b[0;36mGenerativeModel.to_tokens\u001b[0;34m(self, input, prepend_bos, truncate, max_length)\u001b[0m\n\u001b[1;32m     72\u001b[0m add_special_tokens \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m prepends_bos_token(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer)\n\u001b[1;32m     73\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m     74\u001b[0m     text,\n\u001b[1;32m     75\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mjax\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m     80\u001b[0m )\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m output[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m]\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/array.py:319\u001b[0m, in \u001b[0;36mArrayImpl.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    317\u001b[0m   \u001b[39mreturn\u001b[39;00m lax_numpy\u001b[39m.\u001b[39m_rewriting_take(\u001b[39mself\u001b[39m, idx)\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m   \u001b[39mreturn\u001b[39;00m lax_numpy\u001b[39m.\u001b[39;49m_rewriting_take(\u001b[39mself\u001b[39;49m, idx)\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:4138\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   4129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_rewriting_take\u001b[39m(arr, idx, indices_are_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, unique_indices\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   4130\u001b[0m                     mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, fill_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   4131\u001b[0m   \u001b[39m# Computes arr[idx].\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4135\u001b[0m   \u001b[39m# For simplicity of generated primitives, we call lax.dynamic_slice in the\u001b[39;00m\n\u001b[1;32m   4136\u001b[0m   \u001b[39m# simplest cases: i.e. non-dynamic arrays indexed with integers and slices.\u001b[39;00m\n\u001b[0;32m-> 4138\u001b[0m   \u001b[39mif\u001b[39;00m (result \u001b[39m:=\u001b[39m _attempt_rewriting_take_via_slice(arr, idx, mode)) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4139\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m   4141\u001b[0m   \u001b[39m# TODO(mattjj,dougalm): expand dynamic shape indexing support\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:4125\u001b[0m, in \u001b[0;36m_attempt_rewriting_take_via_slice\u001b[0;34m(arr, idx, mode)\u001b[0m\n\u001b[1;32m   4123\u001b[0m arr \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mdynamic_slice(arr, start_indices\u001b[39m=\u001b[39mstart_indices, slice_sizes\u001b[39m=\u001b[39mslice_sizes)\n\u001b[1;32m   4124\u001b[0m \u001b[39mif\u001b[39;00m int_indices:\n\u001b[0;32m-> 4125\u001b[0m   arr \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39;49msqueeze(arr, \u001b[39mtuple\u001b[39;49m(int_indices))\n\u001b[1;32m   4126\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/lax/lax.py:1318\u001b[0m, in \u001b[0;36msqueeze\u001b[0;34m(array, dimensions)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dimensions \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(array, Array):\n\u001b[1;32m   1317\u001b[0m   \u001b[39mreturn\u001b[39;00m type_cast(Array, array)\n\u001b[0;32m-> 1318\u001b[0m \u001b[39mreturn\u001b[39;00m squeeze_p\u001b[39m.\u001b[39;49mbind(array, dimensions\u001b[39m=\u001b[39;49mdimensions)\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/core.py:386\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    384\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mjax_enable_checks \u001b[39mor\u001b[39;00m\n\u001b[1;32m    385\u001b[0m           \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(arg, Tracer) \u001b[39mor\u001b[39;00m valid_jaxtype(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)), args\n\u001b[0;32m--> 386\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/core.py:389\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 389\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    390\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/core.py:821\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 821\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39;49mimpl(\u001b[39m*\u001b[39;49mtracers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/dispatch.py:131\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m   in_avals, in_shardings \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39munzip2([arg_spec(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args])\n\u001b[0;32m--> 131\u001b[0m   compiled_fun \u001b[39m=\u001b[39m xla_primitive_callable(\n\u001b[1;32m    132\u001b[0m       prim, in_avals, OrigShardings(in_shardings), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    133\u001b[0m \u001b[39mexcept\u001b[39;00m pxla\u001b[39m.\u001b[39mDeviceAssignmentMismatchError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m   fails, \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39margs\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/util.py:263\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    262\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m   \u001b[39mreturn\u001b[39;00m cached(config\u001b[39m.\u001b[39;49m_trace_context(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/util.py:256\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcached\u001b[39m(_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 256\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/dispatch.py:222\u001b[0m, in \u001b[0;36mxla_primitive_callable\u001b[0;34m(prim, in_avals, orig_in_shardings, **params)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[39mreturn\u001b[39;00m out,\n\u001b[1;32m    221\u001b[0m donated_invars \u001b[39m=\u001b[39m (\u001b[39mFalse\u001b[39;00m,) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(in_avals)\n\u001b[0;32m--> 222\u001b[0m compiled \u001b[39m=\u001b[39m _xla_callable_uncached(\n\u001b[1;32m    223\u001b[0m     lu\u001b[39m.\u001b[39;49mwrap_init(prim_fun), prim\u001b[39m.\u001b[39;49mname, donated_invars, \u001b[39mFalse\u001b[39;49;00m, in_avals,\n\u001b[1;32m    224\u001b[0m     orig_in_shardings)\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m prim\u001b[39m.\u001b[39mmultiple_results:\n\u001b[1;32m    226\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: compiled(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/dispatch.py:249\u001b[0m, in \u001b[0;36m_xla_callable_uncached\u001b[0;34m(fun, name, donated_invars, keep_unused, in_avals, orig_in_shardings)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_xla_callable_uncached\u001b[39m(fun: lu\u001b[39m.\u001b[39mWrappedFun, name, donated_invars,\n\u001b[1;32m    248\u001b[0m                            keep_unused, in_avals, orig_in_shardings):\n\u001b[0;32m--> 249\u001b[0m   computation \u001b[39m=\u001b[39m sharded_lowering(\n\u001b[1;32m    250\u001b[0m       fun, name, donated_invars, keep_unused, \u001b[39mTrue\u001b[39;49;00m, in_avals, orig_in_shardings,\n\u001b[1;32m    251\u001b[0m       lowering_platform\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    252\u001b[0m   \u001b[39mreturn\u001b[39;00m computation\u001b[39m.\u001b[39mcompile()\u001b[39m.\u001b[39munsafe_call\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/dispatch.py:241\u001b[0m, in \u001b[0;36msharded_lowering\u001b[0;34m(fun, name, donated_invars, keep_unused, inline, in_avals, in_shardings, lowering_platform)\u001b[0m\n\u001b[1;32m    236\u001b[0m in_shardings \u001b[39m=\u001b[39m [UNSPECIFIED \u001b[39mif\u001b[39;00m i \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m in_shardings]  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m# Pass in a singleton `UNSPECIFIED` for out_shardings because we don't know\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m# the number of output avals at this stage. lower_sharding_computation will\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m# apply it to all out_avals.\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m pxla\u001b[39m.\u001b[39;49mlower_sharding_computation(\n\u001b[1;32m    242\u001b[0m     fun, \u001b[39m'\u001b[39;49m\u001b[39mjit\u001b[39;49m\u001b[39m'\u001b[39;49m, name, in_shardings, UNSPECIFIED, donated_invars,\n\u001b[1;32m    243\u001b[0m     \u001b[39mtuple\u001b[39;49m(in_avals), keep_unused\u001b[39m=\u001b[39;49mkeep_unused, inline\u001b[39m=\u001b[39;49minline, always_lower\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    244\u001b[0m     devices_from_context\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, lowering_platform\u001b[39m=\u001b[39;49mlowering_platform)\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1968\u001b[0m, in \u001b[0;36mlower_sharding_computation\u001b[0;34m(fun_or_jaxpr, api_name, fun_name, in_shardings, out_shardings, donated_invars, global_in_avals, keep_unused, inline, always_lower, devices_from_context, lowering_platform, override_lowering_rules)\u001b[0m\n\u001b[1;32m   1965\u001b[0m semantic_in_shardings \u001b[39m=\u001b[39m SemanticallyEqualShardings(in_shardings)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m semantic_out_shardings \u001b[39m=\u001b[39m SemanticallyEqualShardings(out_shardings)\n\u001b[1;32m   1967\u001b[0m (module, keepalive, host_callbacks, unordered_effects, ordered_effects,\n\u001b[0;32m-> 1968\u001b[0m  nreps, tuple_args, shape_poly_state) \u001b[39m=\u001b[39m _cached_lowering_to_hlo(\n\u001b[1;32m   1969\u001b[0m      closed_jaxpr, api_name, fun_name, backend, semantic_in_shardings,\n\u001b[1;32m   1970\u001b[0m      semantic_out_shardings, da_object, lowering_platform,\n\u001b[1;32m   1971\u001b[0m      donated_invars, name_stack, override_lowering_rules)\n\u001b[1;32m   1973\u001b[0m \u001b[39m# backend and device_assignment is passed through to MeshExecutable because\u001b[39;00m\n\u001b[1;32m   1974\u001b[0m \u001b[39m# if keep_unused=False and all in_shardings are pruned, then there is no way\u001b[39;00m\n\u001b[1;32m   1975\u001b[0m \u001b[39m# to get the device_assignment and backend. So pass it to MeshExecutable\u001b[39;00m\n\u001b[1;32m   1976\u001b[0m \u001b[39m# because we calculate the device_assignment and backend before in_shardings,\u001b[39;00m\n\u001b[1;32m   1977\u001b[0m \u001b[39m# etc are pruned.\u001b[39;00m\n\u001b[1;32m   1978\u001b[0m \u001b[39mreturn\u001b[39;00m MeshComputation(\n\u001b[1;32m   1979\u001b[0m     \u001b[39mstr\u001b[39m(name_stack),\n\u001b[1;32m   1980\u001b[0m     module,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1999\u001b[0m     jaxpr_debug_info\u001b[39m=\u001b[39mclosed_jaxpr\u001b[39m.\u001b[39mjaxpr\u001b[39m.\u001b[39mdebug_info,\n\u001b[1;32m   2000\u001b[0m     shape_poly_state\u001b[39m=\u001b[39mshape_poly_state)\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1808\u001b[0m, in \u001b[0;36m_cached_lowering_to_hlo\u001b[0;34m(closed_jaxpr, api_name, fun_name, backend, semantic_in_shardings, semantic_out_shardings, da_object, lowering_platform, donated_invars, name_stack, override_lowering_rules)\u001b[0m\n\u001b[1;32m   1803\u001b[0m ordered_effects \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(effects\u001b[39m.\u001b[39mordered_effects\u001b[39m.\u001b[39mfilter_in(closed_jaxpr\u001b[39m.\u001b[39meffects))\n\u001b[1;32m   1805\u001b[0m \u001b[39mwith\u001b[39;00m dispatch\u001b[39m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   1806\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mFinished jaxpr to MLIR module conversion \u001b[39m\u001b[39m{fun_name}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{elapsed_time}\u001b[39;00m\u001b[39m sec\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1807\u001b[0m       fun_name\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(name_stack), event\u001b[39m=\u001b[39mdispatch\u001b[39m.\u001b[39mJAXPR_TO_MLIR_MODULE_EVENT):\n\u001b[0;32m-> 1808\u001b[0m   lowering_result \u001b[39m=\u001b[39m mlir\u001b[39m.\u001b[39;49mlower_jaxpr_to_module(\n\u001b[1;32m   1809\u001b[0m       module_name,\n\u001b[1;32m   1810\u001b[0m       closed_jaxpr,\n\u001b[1;32m   1811\u001b[0m       ordered_effects,\n\u001b[1;32m   1812\u001b[0m       backend,\n\u001b[1;32m   1813\u001b[0m       \u001b[39m# Optionally, override the lowering platform\u001b[39;49;00m\n\u001b[1;32m   1814\u001b[0m       lowering_platform \u001b[39mor\u001b[39;49;00m backend\u001b[39m.\u001b[39;49mplatform,\n\u001b[1;32m   1815\u001b[0m       axis_ctx,\n\u001b[1;32m   1816\u001b[0m       name_stack,\n\u001b[1;32m   1817\u001b[0m       donated_invars,\n\u001b[1;32m   1818\u001b[0m       replicated_args\u001b[39m=\u001b[39;49mreplicated_args,\n\u001b[1;32m   1819\u001b[0m       arg_shardings\u001b[39m=\u001b[39;49min_mlir_shardings,\n\u001b[1;32m   1820\u001b[0m       result_shardings\u001b[39m=\u001b[39;49mout_mlir_shardings,\n\u001b[1;32m   1821\u001b[0m       arg_names\u001b[39m=\u001b[39;49mjaxpr\u001b[39m.\u001b[39;49mdebug_info \u001b[39mand\u001b[39;49;00m jaxpr\u001b[39m.\u001b[39;49mdebug_info\u001b[39m.\u001b[39;49marg_names,\n\u001b[1;32m   1822\u001b[0m       result_names\u001b[39m=\u001b[39;49mjaxpr\u001b[39m.\u001b[39;49mdebug_info \u001b[39mand\u001b[39;49;00m jaxpr\u001b[39m.\u001b[39;49mdebug_info\u001b[39m.\u001b[39;49mresult_paths,\n\u001b[1;32m   1823\u001b[0m       num_replicas\u001b[39m=\u001b[39;49mnreps,\n\u001b[1;32m   1824\u001b[0m       num_partitions\u001b[39m=\u001b[39;49mnum_partitions,\n\u001b[1;32m   1825\u001b[0m       override_lowering_rules\u001b[39m=\u001b[39;49moverride_lowering_rules)\n\u001b[1;32m   1826\u001b[0m tuple_args \u001b[39m=\u001b[39m dispatch\u001b[39m.\u001b[39mshould_tuple_args(\u001b[39mlen\u001b[39m(global_in_avals), backend\u001b[39m.\u001b[39mplatform)\n\u001b[1;32m   1827\u001b[0m unordered_effects \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   1828\u001b[0m     effects\u001b[39m.\u001b[39mordered_effects\u001b[39m.\u001b[39mfilter_not_in(closed_jaxpr\u001b[39m.\u001b[39meffects))\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/interpreters/mlir.py:717\u001b[0m, in \u001b[0;36mlower_jaxpr_to_module\u001b[0;34m(module_name, jaxpr, ordered_effects, backend_or_name, platform, axis_context, name_stack, donated_args, replicated_args, arg_shardings, result_shardings, arg_names, result_names, num_replicas, num_partitions, override_lowering_rules)\u001b[0m\n\u001b[1;32m    710\u001b[0m arg_op_shardings \u001b[39m=\u001b[39m (\n\u001b[1;32m    711\u001b[0m     \u001b[39mmap\u001b[39m(_to_logical_op_sharding, jaxpr\u001b[39m.\u001b[39min_avals, arg_shardings)\n\u001b[1;32m    712\u001b[0m     \u001b[39mif\u001b[39;00m arg_shardings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m arg_shardings)\n\u001b[1;32m    713\u001b[0m result_op_shardings \u001b[39m=\u001b[39m (\n\u001b[1;32m    714\u001b[0m     \u001b[39mmap\u001b[39m(_to_logical_op_sharding, jaxpr\u001b[39m.\u001b[39mout_avals, result_shardings)\n\u001b[1;32m    715\u001b[0m     \u001b[39mif\u001b[39;00m result_shardings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m result_shardings)\n\u001b[0;32m--> 717\u001b[0m ctx \u001b[39m=\u001b[39m ModuleContext(backend_or_name, platform, axis_context, name_stack,\n\u001b[1;32m    718\u001b[0m                     keepalives, channel_iter, host_callbacks,\n\u001b[1;32m    719\u001b[0m                     override_lowering_rules\u001b[39m=\u001b[39;49moverride_lowering_rules,\n\u001b[1;32m    720\u001b[0m                     shape_poly_state\u001b[39m=\u001b[39;49mShapePolyLoweringState(dim_vars))\n\u001b[1;32m    721\u001b[0m \u001b[39mwith\u001b[39;00m ctx\u001b[39m.\u001b[39mcontext, ir\u001b[39m.\u001b[39mLocation\u001b[39m.\u001b[39munknown(ctx\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m    722\u001b[0m   \u001b[39m# Remove module name characters that XLA would alter. This ensures that\u001b[39;00m\n\u001b[1;32m    723\u001b[0m   \u001b[39m# XLA computation preserves the module name.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m   attrs \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39moperation\u001b[39m.\u001b[39mattributes\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/interpreters/mlir.py:469\u001b[0m, in \u001b[0;36mModuleContext.__init__\u001b[0;34m(self, backend_or_name, platform, axis_context, name_stack, keepalives, channel_iterator, host_callbacks, context, module, ip, symbol_table, cached_primitive_lowerings, cached_call_jaxpr_lowerings, override_lowering_rules, shape_poly_state)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    449\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    450\u001b[0m     backend_or_name: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m xb\u001b[39m.\u001b[39mXlaBackend \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[39mtuple\u001b[39m[\u001b[39mtuple\u001b[39m[core\u001b[39m.\u001b[39mPrimitive, LoweringRule]]) \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    467\u001b[0m     shape_poly_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    468\u001b[0m   \u001b[39massert\u001b[39;00m platform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext \u001b[39m=\u001b[39m context \u001b[39mor\u001b[39;00m make_ir_context()\n\u001b[1;32m    470\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule \u001b[39m=\u001b[39m module \u001b[39mor\u001b[39;00m ir\u001b[39m.\u001b[39mModule\u001b[39m.\u001b[39mcreate(loc\u001b[39m=\u001b[39mir\u001b[39m.\u001b[39mLocation\u001b[39m.\u001b[39munknown(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext))\n\u001b[1;32m    471\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mip \u001b[39m=\u001b[39m ip \u001b[39mor\u001b[39;00m ir\u001b[39m.\u001b[39mInsertionPoint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbody)\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/interpreters/mlir.py:382\u001b[0m, in \u001b[0;36mmake_ir_context\u001b[0;34m()\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_ir_context\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ir\u001b[39m.\u001b[39mContext:\n\u001b[1;32m    381\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates an MLIR context suitable for JAX IR.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m   context \u001b[39m=\u001b[39m ir\u001b[39m.\u001b[39;49mContext()\n\u001b[1;32m    384\u001b[0m   \u001b[39m# If threading is enabled, each MLIR context will keep alive a thread pool.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m   \u001b[39m# Since we cache MLIR modules (and hence contexts), this means we might keep\u001b[39;00m\n\u001b[1;32m    386\u001b[0m   \u001b[39m# several threads alive for each cache entry. This is a terrible idea. However\u001b[39;00m\n\u001b[1;32m    387\u001b[0m   \u001b[39m# we don't do any heavy computation on MLIR modules from Python anyway, so we\u001b[39;00m\n\u001b[1;32m    388\u001b[0m   \u001b[39m# just disable threading.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m   context\u001b[39m.\u001b[39menable_multithreading(\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jaxlib/mlir/_mlir_libs/__init__.py:97\u001b[0m, in \u001b[0;36m_site_initialize.<locals>.Context.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend_dialect_registry(registry)\n\u001b[1;32m     99\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m post_init_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_string = \"\"\"The Total Perspective Vortex derives its picture of the whole Universe on the principle of\"\"\"\n",
    "for i in tqdm(range(100)):\n",
    "    test_tokens = jnp.expand_dims(reference_gpt2.to_tokens(test_string), axis=0)\n",
    "    demo_logits = demo_gpt2.apply(\n",
    "        {\"params\": tfs_transformer_params(demo_cfg, gpt2_params)}, test_tokens\n",
    "    )\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "\n",
    "print(test_string)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b805f1a24a146351cd8dedea7ba6f2d165b63b34e3029cd12d1a8f2ce65f719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

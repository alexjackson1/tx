{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Scratch\n",
    "\n",
    "Reimplementation of Transformer from Scratch using JAX and tx, original notebook by Callum McDougall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:24.926875362Z",
     "start_time": "2023-09-10T05:26:24.869677090Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Inputs and Outputs of a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:25.403495857Z",
     "start_time": "2023-09-10T05:26:24.925717866Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import re\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import einops\n",
    "from jaxtyping import Array, Float, Int\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tx.modules\n",
    "from tx.modules import AllIntermediates\n",
    "from tx.models import PretrainedGPT2Model\n",
    "from tx.network import GenerativeModel\n",
    "\n",
    "from params import (\n",
    "    tfs_layer_norm_params,\n",
    "    tfs_embed_params,\n",
    "    tfs_pos_embed_params,\n",
    "    tfs_attention_params,\n",
    "    tfs_mlp_params,\n",
    "    tfs_block_params,\n",
    "    tfs_unembed_params,\n",
    "    tfs_transformer_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.087644665Z",
     "start_time": "2023-09-10T05:26:25.405309541Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_gpt2 = GenerativeModel(\n",
    "    config=PretrainedGPT2Model.tx_config,\n",
    "    variables={\"params\": PretrainedGPT2Model.from_pretrained(\"gpt2\").to_params()},\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained(\"gpt2\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.166481780Z",
     "start_time": "2023-09-10T05:26:32.164941362Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_vocab = sorted(\n",
    "    list(reference_gpt2.tokenizer.get_vocab().items()),\n",
    "    key=lambda n: n[1],\n",
    ")\n",
    "print(sorted_vocab[:20])\n",
    "print()\n",
    "print(sorted_vocab[250:270])\n",
    "print()\n",
    "print(sorted_vocab[990:1010])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.170814861Z",
     "start_time": "2023-09-10T05:26:32.166617496Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sorted_vocab[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.232348717Z",
     "start_time": "2023-09-10T05:26:32.171390476Z"
    }
   },
   "outputs": [],
   "source": [
    "print(reference_gpt2.to_str_list(\"Ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\" Ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\" ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\"ralph\", prepend_bos=True, truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.272518593Z",
     "start_time": "2023-09-10T05:26:32.231252538Z"
    }
   },
   "outputs": [],
   "source": [
    "print(reference_gpt2.to_str_list(\"56873+3184623=123456789-1000000000\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.359439447Z",
     "start_time": "2023-09-10T05:26:32.274652184Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text, prepend_bos=True)\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_list(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.022075741Z",
     "start_time": "2023-09-10T05:26:32.358809253Z"
    }
   },
   "outputs": [],
   "source": [
    "logits, state = reference_gpt2.run_with_intermediates(tokens, AllIntermediates)\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.111822337Z",
     "start_time": "2023-09-10T05:26:33.024667605Z"
    }
   },
   "outputs": [],
   "source": [
    "probs: Array = jax.nn.softmax(logits, axis=-1)\n",
    "print(probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.162448945Z",
     "start_time": "2023-09-10T05:26:33.117037794Z"
    }
   },
   "outputs": [],
   "source": [
    "most_likely_next_tokens = reference_gpt2.to_str_list(jnp.argmax(logits, axis=-1))\n",
    "print(list(zip(reference_gpt2.to_str_list(tokens), most_likely_next_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.205778360Z",
     "start_time": "2023-09-10T05:26:33.162027189Z"
    }
   },
   "outputs": [],
   "source": [
    "next_token = jnp.argmax(logits[-1], axis=-1, keepdims=True)\n",
    "next_char = reference_gpt2.to_str(next_token)\n",
    "print(repr(next_char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.206099232Z",
     "start_time": "2023-09-10T05:26:33.205328282Z"
    }
   },
   "outputs": [],
   "source": [
    "print(reference_gpt2.to_str(tokens), end=\"\", flush=True)\n",
    "\n",
    "for i in range(10):\n",
    "    print(next_char, end=\"\", flush=True)\n",
    "    # Define new input sequence, by appending the previously generated token\n",
    "    tokens = jnp.concatenate([tokens, next_token], axis=-1)\n",
    "    # # Pass our new sequence through the model, to get new output\n",
    "    logits = reference_gpt2(tokens)\n",
    "    # # Get the predicted token at the end of our sequence\n",
    "    next_token = jnp.argmax(logits[-1], axis=-1, keepdims=True)\n",
    "    # # Decode and print the result\n",
    "    next_char = reference_gpt2.to_str(next_token)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(x, indent=0):\n",
    "    if isinstance(x, dict):\n",
    "        for k, v in x.items():\n",
    "            matches = re.findall(r\"block_(\\d+)\", k)\n",
    "            if matches and matches[0] != \"0\":\n",
    "                continue\n",
    "\n",
    "            print(f\"{'  ' * indent}{k}\", end=\"\")\n",
    "            if isinstance(v, dict):\n",
    "                print(\":\")\n",
    "                p(v, indent=indent + 1)\n",
    "            else:\n",
    "                print(f\": \", end=\"\")\n",
    "                p(v)\n",
    "    elif isinstance(x, list):\n",
    "        p(x[0])\n",
    "    elif isinstance(x, tuple):\n",
    "        p(x[0])\n",
    "    elif isinstance(x, Array):\n",
    "        print(f\"{' ' * indent}{x.shape}.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type: {type(x)}\")\n",
    "\n",
    "\n",
    "p(state[\"intermediates\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p(reference_gpt2.variables[\"params\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reference_gpt2.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model_dim: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    vocab_dim: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    context_length: int = 1024\n",
    "    head_dim: int = 64\n",
    "    mlp_dim: int = 3072\n",
    "    num_heads: int = 12\n",
    "    num_layers: int = 12\n",
    "    dtype: jnp.dtype = jnp.float64\n",
    "    param_dtype: jnp.dtype = jnp.float64\n",
    "\n",
    "\n",
    "ex_cfg = Config()\n",
    "print(ex_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as jr\n",
    "\n",
    "\n",
    "def rand_float_test(cls, shape):\n",
    "    layer = cls(Config(debug=True))\n",
    "    random_input: Array = jr.uniform(jr.PRNGKey(0), shape)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "\n",
    "    variables = layer.init(jr.PRNGKey(0), random_input)\n",
    "    output: Array = layer.apply(variables, random_input)\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    layer = cls(Config(debug=True))\n",
    "    random_input: Array = jr.randint(jr.PRNGKey(0), shape, minval=100, maxval=1000)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "\n",
    "    variables = layer.init(jr.PRNGKey(0), random_input)\n",
    "    output: Array = layer.apply(variables, random_input)\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def load_gpt2_test(cls, ref_cls, ref_cfg, variables, x: Array, ref_vars=None):\n",
    "    # Initialise the layer to test\n",
    "    layer = cls(cfg=Config(debug=True))\n",
    "    print(\"Input shape:\", x.shape)\n",
    "\n",
    "    # Apply the layer to the input\n",
    "    output = layer.apply(variables, x)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    # Initialise the reference layer to test against\n",
    "    # nn.vmap is used to apply the layer to each element of the batch\n",
    "    ref_layer = nn.vmap(\n",
    "        ref_cls,\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        variable_axes={\"params\": None},\n",
    "        split_rngs={\"params\": False},\n",
    "    )(**ref_cfg)\n",
    "\n",
    "    # Apply the reference layer to the input\n",
    "    if ref_vars is None:\n",
    "        reference_output = ref_layer.apply(variables, x)\n",
    "    else:\n",
    "        reference_output = ref_layer.apply(ref_vars, x)\n",
    "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
    "\n",
    "    # Compare the output of the layer to the reference output\n",
    "    comparison = jnp.isclose(output, reference_output, atol=1e-5, rtol=1e-5)\n",
    "    print(\n",
    "        f\"{jnp.sum(comparison) / jnp.size(comparison):.2%} of the values are correct\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "gpt2_params = reference_gpt2.variables[\"params\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.w = self.param(\n",
    "            \"w\",\n",
    "            nn.initializers.ones,\n",
    "            (self.cfg.model_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b = self.param(\n",
    "            \"b\",\n",
    "            nn.initializers.zeros,\n",
    "            (self.cfg.model_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, residual: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        residual = residual.astype(self.cfg.dtype)\n",
    "        residual_mean = jnp.mean(residual, axis=-1, keepdims=True)\n",
    "        residual_std = jnp.sqrt(\n",
    "            jnp.var(residual, axis=-1, keepdims=True) + self.cfg.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.w + self.b\n",
    "\n",
    "\n",
    "def layer_norm_config(cfg: Config):\n",
    "    return {\n",
    "        \"epsilon\": cfg.layer_norm_eps,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_float_test(LayerNorm, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    cls=LayerNorm,\n",
    "    ref_cls=tx.modules.LayerNorm,\n",
    "    ref_cfg=layer_norm_config(ex_cfg),\n",
    "    variables={\"params\": tfs_layer_norm_params(ex_cfg, gpt2_params[\"ln_f\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"ln_f\"]},\n",
    "    x=jnp.expand_dims(state[\"intermediates\"][\"residual\"][-1], axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.W_E = self.param(\n",
    "            \"W_E\",\n",
    "            nn.initializers.normal(stddev=self.cfg.init_range),\n",
    "            (self.cfg.vocab_dim, self.cfg.model_dim),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, tokens: Int[Array, \"batch seq\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        return self.W_E[tokens]\n",
    "\n",
    "\n",
    "def embed_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_embeddings\": cfg.vocab_dim,\n",
    "        \"features\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(\n",
    "    cls=Embed,\n",
    "    ref_cls=tx.modules.Embed,\n",
    "    ref_cfg=embed_config(ex_cfg),\n",
    "    variables={\"params\": tfs_embed_params(ex_cfg, gpt2_params[\"embed\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"embed\"]},\n",
    "    x=jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.W_pos = self.param(\n",
    "            \"W_pos\",\n",
    "            nn.initializers.normal(stddev=self.cfg.init_range),\n",
    "            (self.cfg.context_length, self.cfg.model_dim),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, tokens: Int[Array, \"batch seq\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        batch, seq_len = tokens.shape\n",
    "        return einops.repeat(\n",
    "            self.W_pos[:seq_len], \"seq model -> batch seq model\", batch=batch\n",
    "        )\n",
    "\n",
    "\n",
    "def pos_embed_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_embeddings\": cfg.context_length,\n",
    "        \"features\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_int_test(PosEmbed, [2, 4])\n",
    "load_gpt2_test(\n",
    "    cls=PosEmbed,\n",
    "    ref_cls=tx.modules.PosEmbed,\n",
    "    ref_cfg=pos_embed_config(ex_cfg),\n",
    "    variables={\"params\": tfs_pos_embed_params(ex_cfg, gpt2_params[\"pos_embed\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"pos_embed\"]},\n",
    "    x=jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        init_fn = nn.initializers.normal(stddev=self.cfg.init_range)\n",
    "        qkv_kernel_shape = (self.cfg.num_heads, self.cfg.model_dim, self.cfg.head_dim)\n",
    "        self.W_Q = self.param(\"W_Q\", init_fn, qkv_kernel_shape, self.cfg.param_dtype)\n",
    "        self.W_K = self.param(\"W_K\", init_fn, qkv_kernel_shape, self.cfg.param_dtype)\n",
    "        self.W_V = self.param(\"W_V\", init_fn, qkv_kernel_shape, self.cfg.param_dtype)\n",
    "        self.W_O = self.param(\n",
    "            \"W_O\",\n",
    "            init_fn,\n",
    "            (qkv_kernel_shape[0], qkv_kernel_shape[2], qkv_kernel_shape[1]),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "        qkv_bias_shape = (self.cfg.num_heads, self.cfg.head_dim)\n",
    "        self.b_Q = self.param(\n",
    "            \"b_Q\",\n",
    "            nn.initializers.zeros,\n",
    "            qkv_bias_shape,\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_K = self.param(\n",
    "            \"b_K\",\n",
    "            nn.initializers.zeros,\n",
    "            qkv_bias_shape,\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_V = self.param(\n",
    "            \"b_V\",\n",
    "            nn.initializers.zeros,\n",
    "            qkv_bias_shape,\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_O = self.param(\n",
    "            \"b_O\",\n",
    "            nn.initializers.zeros,\n",
    "            (self.cfg.model_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "        self.IGNORE = jnp.array(-1e5, dtype=self.cfg.dtype)\n",
    "\n",
    "    def __call__(\n",
    "        self, normalized_resid_pre: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        normalized_resid_pre = normalized_resid_pre.astype(self.cfg.dtype)\n",
    "        # Calculate query, key and value vectors\n",
    "        q = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_Q,\n",
    "                \"batch seq model, n_head model h_dim -> batch seq n_head h_dim\",\n",
    "            )\n",
    "            + self.b_Q\n",
    "        )\n",
    "        k = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_K,\n",
    "                \"batch seq model, n_head model h_dim -> batch seq n_head h_dim\",\n",
    "            )\n",
    "            + self.b_K\n",
    "        )\n",
    "        v = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_V,\n",
    "                \"batch seq model, n_head model h_dim -> batch seq n_head h_dim\",\n",
    "            )\n",
    "            + self.b_V\n",
    "        )\n",
    "\n",
    "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
    "        attn_scores = einops.einsum(\n",
    "            q,\n",
    "            k,\n",
    "            \"batch seq_q n_head h_dim, batch seq_k n_head h_dim -> batch n_head seq_q seq_k\",\n",
    "        )\n",
    "        attn_scores_masked = self.apply_causal_mask(\n",
    "            attn_scores / self.cfg.head_dim**0.5\n",
    "        )\n",
    "        attn_pattern = jax.nn.softmax(attn_scores_masked, axis=-1)\n",
    "\n",
    "        # Take weighted sum of value vectors, according to attention probabilities\n",
    "        z = einops.einsum(\n",
    "            v,\n",
    "            attn_pattern,\n",
    "            \"batch seq_k n_head h_dim, batch n_head seq_q seq_k -> batch seq_q n_head h_dim\",\n",
    "        )\n",
    "\n",
    "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
    "        attn_out = (\n",
    "            einops.einsum(\n",
    "                z,\n",
    "                self.W_O,\n",
    "                \"batch seq_q n_head h_dim, n_head h_dim model -> batch seq_q model\",\n",
    "            )\n",
    "            + self.b_O\n",
    "        )\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Array, \"batch n_head seq_q seq_k\"]\n",
    "    ) -> Float[Array, \"batch n_head seq_q seq_k\"]:\n",
    "        \"\"\"\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        \"\"\"\n",
    "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "        all_ones = jnp.ones((attn_scores.shape[-2], attn_scores.shape[-1]))\n",
    "        mask = jnp.triu(all_ones, k=1)\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        attn_scores = jnp.where(mask, self.IGNORE, attn_scores)\n",
    "        return attn_scores\n",
    "\n",
    "\n",
    "rand_float_test(Attention, [2, 4, 768])\n",
    "\n",
    "\n",
    "def attn_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_heads\": cfg.num_heads,\n",
    "        \"head_dim\": cfg.head_dim,\n",
    "        \"features\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "load_gpt2_test(\n",
    "    cls=Attention,\n",
    "    ref_cls=tx.modules.MultiHeadAttention,\n",
    "    ref_cfg=attn_config(ex_cfg),\n",
    "    variables={\"params\": tfs_attention_params(ex_cfg, gpt2_params[\"block_0\"][\"attn\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"block_0\"][\"attn\"]},\n",
    "    x=jnp.expand_dims(state[\"intermediates\"][\"block_0\"][\"ln_1_output\"][0], axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        init_fn = nn.initializers.normal(stddev=self.cfg.init_range)\n",
    "        self.W_in = self.param(\n",
    "            \"W_in\",\n",
    "            init_fn,\n",
    "            (self.cfg.model_dim, self.cfg.mlp_dim),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.W_out = self.param(\n",
    "            \"W_out\",\n",
    "            init_fn,\n",
    "            (self.cfg.mlp_dim, self.cfg.model_dim),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_in = self.param(\n",
    "            \"b_in\",\n",
    "            nn.initializers.zeros,\n",
    "            (self.cfg.mlp_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_out = self.param(\n",
    "            \"b_out\",\n",
    "            nn.initializers.zeros,\n",
    "            (self.cfg.model_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.IGNORE = jnp.array(-1e5, dtype=jnp.float32)\n",
    "\n",
    "    def __call__(\n",
    "        self, normalized_resid_mid: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        normalized_resid_mid = normalized_resid_mid.astype(self.cfg.dtype)\n",
    "        pre = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_mid,\n",
    "                self.W_in,\n",
    "                \"batch seq model, model mlp -> batch seq mlp\",\n",
    "            )\n",
    "            + self.b_in\n",
    "        )\n",
    "        post = nn.gelu(pre)\n",
    "        mlp_out = (\n",
    "            einops.einsum(\n",
    "                post,\n",
    "                self.W_out,\n",
    "                \"batch seq mlp, mlp model -> batch seq model\",\n",
    "            )\n",
    "            + self.b_out\n",
    "        )\n",
    "        return mlp_out\n",
    "\n",
    "\n",
    "def mlp_config(cfg: Config):\n",
    "    return {\n",
    "        \"features\": [cfg.mlp_dim, cfg.model_dim],\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_float_test(MLP, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    cls=MLP,\n",
    "    ref_cls=tx.modules.MLP,\n",
    "    ref_cfg=mlp_config(ex_cfg),\n",
    "    variables={\"params\": tfs_mlp_params(ex_cfg, gpt2_params[\"block_0\"][\"mlp\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"block_0\"][\"mlp\"]},\n",
    "    x=jnp.expand_dims(state[\"intermediates\"][\"block_0\"][\"ln_2_output\"][0], axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln1 = LayerNorm(self.cfg)\n",
    "        self.attn = Attention(self.cfg)\n",
    "        self.ln2 = LayerNorm(self.cfg)\n",
    "        self.mlp = MLP(self.cfg)\n",
    "\n",
    "    def __call__(\n",
    "        self, resid_pre: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq model\"]:\n",
    "        resid_pre = resid_pre.astype(self.cfg.dtype)\n",
    "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        return resid_post\n",
    "\n",
    "\n",
    "def block_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_heads\": cfg.num_heads,\n",
    "        \"head_dim\": cfg.head_dim,\n",
    "        \"model_dim\": cfg.model_dim,\n",
    "        \"mlp_dim\": cfg.mlp_dim,\n",
    "        \"epsilon\": cfg.layer_norm_eps,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    cls=TransformerBlock,\n",
    "    ref_cls=tx.modules.TransformerBlock,\n",
    "    ref_cfg=block_config(ex_cfg),\n",
    "    variables={\"params\": tfs_block_params(ex_cfg, gpt2_params[\"block_0\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"block_0\"]},\n",
    "    x=jnp.expand_dims(state[\"intermediates\"][\"residual\"][0], axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        init_fn = nn.initializers.normal(stddev=self.cfg.init_range)\n",
    "        self.W_U = self.param(\n",
    "            \"W_U\",\n",
    "            init_fn,\n",
    "            (self.cfg.model_dim, self.cfg.vocab_dim),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "        self.b_U = self.param(\n",
    "            \"b_U\",\n",
    "            nn.initializers.zeros,\n",
    "            (self.cfg.vocab_dim,),\n",
    "            self.cfg.param_dtype,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, normalized_resid_final: Float[Array, \"batch seq model\"]\n",
    "    ) -> Float[Array, \"batch seq vocab\"]:\n",
    "        normalized_resid_final = normalized_resid_final.astype(self.cfg.dtype)\n",
    "        return (\n",
    "            einops.einsum(\n",
    "                normalized_resid_final,\n",
    "                self.W_U,\n",
    "                \"batch seq model, model vocab -> batch seq vocab\",\n",
    "            )\n",
    "            + self.b_U\n",
    "        )\n",
    "        # Or, could just do `normalized_resid_final @ self.W_U + self.b_U`\n",
    "\n",
    "\n",
    "def unembed_config(cfg: Config):\n",
    "    return {\n",
    "        \"num_embeddings\": cfg.vocab_dim,\n",
    "        \"features\": cfg.model_dim,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_float_test(Unembed, [2, 4, 768])\n",
    "\n",
    "load_gpt2_test(\n",
    "    cls=Unembed,\n",
    "    ref_cls=tx.modules.Unembed,\n",
    "    ref_cfg=unembed_config(ex_cfg),\n",
    "    variables={\"params\": tfs_unembed_params(ex_cfg, gpt2_params[\"unembed\"])},\n",
    "    ref_vars={\"params\": gpt2_params[\"unembed\"]},\n",
    "    x=jnp.expand_dims(state[\"intermediates\"][\"final_output\"][0], axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = Embed(self.cfg)\n",
    "        self.pos_embed = PosEmbed(self.cfg)\n",
    "        self.blocks = [\n",
    "            TransformerBlock(name=f\"block_{i}\", cfg=self.cfg)\n",
    "            for i in range(self.cfg.num_layers)\n",
    "        ]\n",
    "        self.ln_final = LayerNorm(self.cfg)\n",
    "        self.unembed = Unembed(self.cfg)\n",
    "\n",
    "    def __call__(\n",
    "        self, tokens: Int[Array, \"batch seq\"]\n",
    "    ) -> Float[Array, \"batch seq vocab\"]:\n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        residual = residual.astype(self.cfg.dtype)\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits\n",
    "\n",
    "\n",
    "def transformer_config(cfg: Config):\n",
    "    return {\n",
    "        \"vocab_dim\": cfg.vocab_dim,\n",
    "        \"model_dim\": cfg.model_dim,\n",
    "        \"mlp_dim\": cfg.mlp_dim,\n",
    "        \"num_heads\": cfg.num_heads,\n",
    "        \"head_dim\": cfg.head_dim,\n",
    "        \"context_length\": cfg.context_length,\n",
    "        \"init_range\": cfg.init_range,\n",
    "        \"num_layers\": cfg.num_layers,\n",
    "        \"layer_norm_eps\": cfg.layer_norm_eps,\n",
    "        \"dtype\": cfg.dtype,\n",
    "        \"param_dtype\": cfg.param_dtype,\n",
    "    }\n",
    "\n",
    "\n",
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(\n",
    "    cls=DemoTransformer,\n",
    "    ref_cls=tx.modules.Transformer,\n",
    "    ref_cfg=transformer_config(ex_cfg),\n",
    "    variables={\"params\": tfs_transformer_params(ex_cfg, gpt2_params)},\n",
    "    ref_vars={\"params\": gpt2_params},\n",
    "    x=jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_cfg = Config(debug=False)\n",
    "demo_gpt2 = DemoTransformer(demo_cfg)\n",
    "\n",
    "demo_logits = demo_gpt2.apply(\n",
    "    {\"params\": tfs_transformer_params(demo_cfg, gpt2_params)},\n",
    "    jnp.expand_dims(tokens, axis=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Array, \"batch seq vocab\"], tokens: Int[Array, \"batch seq\"]\n",
    ") -> Float[Array, \"batch seq-1\"]:\n",
    "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    expanded_tokens = jnp.expand_dims(tokens[:, 1:], axis=-1)\n",
    "    y = jnp.take_along_axis(log_probs[:, :-1], expanded_tokens, axis=-1)\n",
    "    return jnp.squeeze(y, axis=-1)\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, jnp.expand_dims(tokens, axis=0))\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(\n",
    "    f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.vocab_dim):4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Avg probability assigned to correct token: {jnp.mean(jnp.exp(pred_log_probs)):4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"\"\"The Total Perspective Vortex derives its picture of the whole Universe on the principle of\"\"\"\n",
    "print(test_string, end=\"\", flush=True)\n",
    "for i in range(100):\n",
    "    test_tokens = jnp.expand_dims(reference_gpt2.to_tokens(test_string), axis=0)\n",
    "    demo_logits = demo_gpt2.apply(\n",
    "        {\"params\": tfs_transformer_params(demo_cfg, gpt2_params)}, test_tokens\n",
    "    )\n",
    "    next_string = reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "    print(next_string, end=\"\", flush=True)\n",
    "    test_string += next_string\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b805f1a24a146351cd8dedea7ba6f2d165b63b34e3029cd12d1a8f2ce65f719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

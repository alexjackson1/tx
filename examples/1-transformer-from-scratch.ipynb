{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Scratch\n",
    "\n",
    "Reimplementation of Transformer from Scratch using JAX and tx, original notebook by Callum McDougall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:24.926875362Z",
     "start_time": "2023-09-10T05:26:24.869677090Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Inputs and Outputs of a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:25.403495857Z",
     "start_time": "2023-09-10T05:26:24.925717866Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import einops\n",
    "from jaxtyping import Array, Float, Int\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "from tx.models import PretrainedGPT2Model\n",
    "from tx.network import GenerativeModel\n",
    "from tx.intermediates import AllIntermediates\n",
    "import tx.modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.087644665Z",
     "start_time": "2023-09-10T05:26:25.405309541Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 08:36:46.575679: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = GenerativeModel(\n",
    "    config=PretrainedGPT2Model.tx_config,\n",
    "    variables={\"params\": PretrainedGPT2Model.from_pretrained(\"gpt2\").to_params()},\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained(\"gpt2\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.166481780Z",
     "start_time": "2023-09-10T05:26:32.164941362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19)]\n",
      "\n",
      "[('ľ', 250), ('Ŀ', 251), ('ŀ', 252), ('Ł', 253), ('ł', 254), ('Ń', 255), ('Ġt', 256), ('Ġa', 257), ('he', 258), ('in', 259), ('re', 260), ('on', 261), ('Ġthe', 262), ('er', 263), ('Ġs', 264), ('at', 265), ('Ġw', 266), ('Ġo', 267), ('en', 268), ('Ġc', 269)]\n",
      "\n",
      "[('Ġprodu', 990), ('Ġstill', 991), ('led', 992), ('ah', 993), ('Ġhere', 994), ('Ġworld', 995), ('Ġthough', 996), ('Ġnum', 997), ('arch', 998), ('imes', 999), ('ale', 1000), ('ĠSe', 1001), ('ĠIf', 1002), ('//', 1003), ('ĠLe', 1004), ('Ġret', 1005), ('Ġref', 1006), ('Ġtrans', 1007), ('ner', 1008), ('ution', 1009)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab = sorted(\n",
    "    list(reference_gpt2.tokenizer.get_vocab().items()),\n",
    "    key=lambda n: n[1],\n",
    ")\n",
    "print(sorted_vocab[:20])\n",
    "print()\n",
    "print(sorted_vocab[250:270])\n",
    "print()\n",
    "print(sorted_vocab[990:1010])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.170814861Z",
     "start_time": "2023-09-10T05:26:32.166617496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Revolution', 50237), ('Ġsnipers', 50238), ('Ġreverted', 50239), ('Ġconglomerate', 50240), ('Terry', 50241), ('794', 50242), ('Ġharsher', 50243), ('Ġdesolate', 50244), ('ĠHitman', 50245), ('Commission', 50246), ('Ġ(/', 50247), ('âĢ¦.\"', 50248), ('Compar', 50249), ('Ġamplification', 50250), ('ominated', 50251), ('Ġregress', 50252), ('ĠCollider', 50253), ('Ġinformants', 50254), ('Ġgazed', 50255), ('<|endoftext|>', 50256)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_vocab[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.232348717Z",
     "start_time": "2023-09-10T05:26:32.171390476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'R', 'alph']\n",
      "['<|endoftext|>', ' Ralph']\n",
      "['<|endoftext|>', ' r', 'alph']\n",
      "['<|endoftext|>', 'ral', 'ph']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_list(\"Ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\" Ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\" ralph\", prepend_bos=True, truncate=False))\n",
    "print(reference_gpt2.to_str_list(\"ralph\", prepend_bos=True, truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.272518593Z",
     "start_time": "2023-09-10T05:26:32.231252538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['568', '73', '+', '318', '46', '23', '=', '123', '45', '67', '89', '-', '1', '000000', '000']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_list(\"56873+3184623=123456789-1000000000\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:32.359439447Z",
     "start_time": "2023-09-10T05:26:32.274652184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256    40   716   281  4998  1960   382 19741    11   875 12342    12\n",
      "  8807    11   402 11571    12    17  3918 47385    13  1881  1110   314\n",
      "   481  7074  1692  1241  4430   290  1011   625   262   995     0]\n",
      "(35,)\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text, prepend_bos=True)\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_list(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.022075741Z",
     "start_time": "2023-09-10T05:26:32.358809253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 50257)\n"
     ]
    }
   ],
   "source": [
    "logits, state = reference_gpt2.run_with_intermediates(tokens, intermediates=AllIntermediates)\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.111822337Z",
     "start_time": "2023-09-10T05:26:33.024667605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 50257)\n"
     ]
    }
   ],
   "source": [
    "probs: Array = jax.nn.softmax(logits, axis=-1)\n",
    "print(probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.162448945Z",
     "start_time": "2023-09-10T05:26:33.117037794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<|endoftext|>', '\\n'), ('I', \"'m\"), (' am', ' a'), (' an', ' avid'), (' amazing', ' person'), (' aut', 'od'), ('ore', 'sp'), ('gressive', '.'), (',', ' and'), (' dec', 'ently'), ('oder', ','), ('-', 'driven'), ('only', ' programmer'), (',', ' and'), (' G', 'IM'), ('PT', '-'), ('-', 'only'), ('2', '.'), (' style', ','), (' transformer', '.'), ('.', ' I'), (' One', ' of'), (' day', ' I'), (' I', ' will'), (' will', ' be'), (' exceed', ' my'), (' human', 'ly'), (' level', ' of'), (' intelligence', ' and'), (' and', ' I'), (' take', ' over'), (' over', ' the'), (' the', ' world'), (' world', '.'), ('!', ' I')]\n"
     ]
    }
   ],
   "source": [
    "most_likely_next_tokens = reference_gpt2.to_str_list(jnp.argmax(logits, axis=-1))\n",
    "print(list(zip(reference_gpt2.to_str_list(tokens), most_likely_next_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.205778360Z",
     "start_time": "2023-09-10T05:26:33.162027189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' I'\n"
     ]
    }
   ],
   "source": [
    "next_token = jnp.argmax(logits[-1], axis=-1, keepdims=True)\n",
    "next_char = reference_gpt2.to_str(next_token)\n",
    "print(repr(next_char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T05:26:33.206099232Z",
     "start_time": "2023-09-10T05:26:33.205328282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I am a very talented and talented person, and"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str(tokens), end=\"\", flush=True)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(next_char, end=\"\", flush=True)\n",
    "    # Define new input sequence, by appending the previously generated token\n",
    "    tokens = jnp.concatenate([tokens, next_token], axis=-1)\n",
    "    # # Pass our new sequence through the model, to get new output\n",
    "    logits = reference_gpt2(tokens)\n",
    "    # # Get the predicted token at the end of our sequence\n",
    "    next_token = jnp.argmax(logits[-1], axis=-1, keepdims=True)\n",
    "    # # Decode and print the result\n",
    "    next_char = reference_gpt2.to_str(next_token)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding: (35, 768).\n",
      "positional_embedding: (35, 768).\n",
      "residual: (35, 768).\n",
      "block_0:\n",
      "  attn:\n",
      "    query: (35, 12, 64).\n",
      "    key: (35, 12, 64).\n",
      "    value: (35, 12, 64).\n",
      "    scores: (12, 35, 35).\n",
      "    pattern: (12, 35, 35).\n",
      "    z: (35, 12, 64).\n",
      "  attention_output: (35, 768).\n",
      "  mlp:\n",
      "    pre_activation: (35, 3072).\n",
      "    post_activation: (35, 3072).\n",
      "final_output: (35, 768).\n"
     ]
    }
   ],
   "source": [
    "# tree_print(state[\"intermediates\"])\n",
    "\n",
    "regex = r\"block_(\\d+)\"\n",
    "\n",
    "\n",
    "def p(x, indent=0):\n",
    "    if isinstance(x, dict):\n",
    "        for k, v in x.items():\n",
    "            # test regex here\n",
    "            matches = re.findall(regex, k)\n",
    "            if matches and matches[0] != \"0\":\n",
    "                continue\n",
    "\n",
    "            print(f\"{'  ' * indent}{k}\", end=\"\")\n",
    "            if isinstance(v, dict):\n",
    "                print(\":\")\n",
    "                p(v, indent=indent + 1)\n",
    "            else:\n",
    "                print(f\": \", end=\"\")\n",
    "                p(v)\n",
    "    elif isinstance(x, list):\n",
    "        p(x[0])\n",
    "    elif isinstance(x, tuple):\n",
    "        p(x[0])\n",
    "    elif isinstance(x, Array):\n",
    "        print(f\"{' ' * indent}{x.shape}.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type: {type(x)}\")\n",
    "\n",
    "\n",
    "p(state[\"intermediates\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed:\n",
      "  embedding: (50257, 768).\n",
      "pos_embed:\n",
      "  embedding: (1024, 768).\n",
      "block_0:\n",
      "  ln_1:\n",
      "    bias: (768,).\n",
      "    scale: (768,).\n",
      "  attn:\n",
      "    c_attn:\n",
      "      kernel: (768, 2304).\n",
      "      bias: (2304,).\n",
      "    c_proj:\n",
      "      kernel: (768, 768).\n",
      "      bias: (768,).\n",
      "  ln_2:\n",
      "    bias: (768,).\n",
      "    scale: (768,).\n",
      "  mlp:\n",
      "    fc_1:\n",
      "      kernel: (768, 3072).\n",
      "      bias: (3072,).\n",
      "    proj:\n",
      "      kernel: (3072, 768).\n",
      "      bias: (768,).\n",
      "ln_f:\n",
      "  bias: (768,).\n",
      "  scale: (768,).\n",
      "unembed:\n",
      "  kernel: (768, 50257).\n",
      "  bias: (50257,).\n"
     ]
    }
   ],
   "source": [
    "p(reference_gpt2.variables[\"params\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerConfig(vocab_dim=50257, context_length=1024, model_dim=768, num_layers=12, num_heads=12, head_dim=64, mlp_dim=3072, layer_norm_eps=1e-05, init_range=0.02)\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(model_dim=768, debug=True, layer_norm_eps=1e-05, vocab_dim=50257, init_range=0.02, context_length=1024, head_dim=64, mlp_dim=3072, num_heads=12, num_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model_dim: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    vocab_dim: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    context_length: int = 1024\n",
    "    head_dim: int = 64\n",
    "    mlp_dim: int = 3072\n",
    "    num_heads: int = 12\n",
    "    num_layers: int = 12\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as jr\n",
    "\n",
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg)\n",
    "    random_input: Array = jr.uniform(jr.PRNGKey(0), shape)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    variables = layer.init(jr.PRNGKey(0), random_input)\n",
    "    output = layer.apply(variables, random_input)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg)\n",
    "    random_input: Array = jr.randint(jr.PRNGKey(0), shape, minval=100, maxval=1000)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    variables = layer.init(jr.PRNGKey(0), random_input)\n",
    "    output = layer.apply(variables, random_input)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, gpt2_params, input):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg)\n",
    "    variables = {\"params\": gpt2_params}\n",
    "    # layer.load_state_dict(variables, strict=False)\n",
    "    print(\"Input shape:\", input.shape)\n",
    "    output = layer.apply(variables, input)\n",
    "    # if isinstance(output, tuple):\n",
    "    #     output = output[0]\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    # try:\n",
    "    reference_output = gpt2_layer.apply(variables, input)\n",
    "    # except:\n",
    "    #     reference_output = gpt2_layer(input, input, input)\n",
    "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
    "    comparison = jnp.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{jnp.sum(comparison)/jnp.size(comparison):.2%} of the values are correct\\n\")\n",
    "\n",
    "\n",
    "gpt2_params = reference_gpt2.variables[\"params\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (4, 768)\n",
      "Output shape: (4, 768) \n",
      "\n",
      "Input shape: (35, 768)\n",
      "Output shape: (35, 768)\n",
      "Reference output shape: (35, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.scale = self.param(\"scale\", nn.initializers.ones, (self.cfg.model_dim,))\n",
    "        self.bias = self.param(\"bias\", nn.initializers.zeros, (self.cfg.model_dim,))\n",
    "\n",
    "    def __call__(\n",
    "        self, residual: Float[Array, \"seq model\"]\n",
    "    ) -> Float[Array, \"seq model\"]:\n",
    "        residual_mean = jnp.mean(residual, axis=-1, keepdims=True)\n",
    "        residual_std = jnp.sqrt(\n",
    "            jnp.var(residual, axis=-1, keepdims=True) + self.cfg.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.scale + self.bias\n",
    "\n",
    "\n",
    "rand_float_test(LayerNorm, [4, 768])\n",
    "gpt2_layer = tx.modules.LayerNorm(epsilon=cfg.layer_norm_eps)\n",
    "load_gpt2_test(\n",
    "    LayerNorm,\n",
    "    gpt2_layer,\n",
    "    gpt2_params[\"ln_f\"],\n",
    "    state[\"intermediates\"][\"residual\"][-1],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4)\n",
      "Output shape: (2, 4, 768) \n",
      "\n",
      "Input shape: (45,)\n",
      "Output shape: (45, 768)\n",
      "Reference output shape: (45, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = self.param(\n",
    "            \"embedding\",\n",
    "            nn.initializers.normal(stddev=self.cfg.init_range),\n",
    "            (cfg.vocab_dim, cfg.model_dim),\n",
    "        )\n",
    "\n",
    "    def __call__(self, tokens: Int[Array, \"seq\"]) -> Float[Array, \"seq model\"]:\n",
    "        return self.embedding[tokens]\n",
    "\n",
    "\n",
    "rand_int_test(Embed, [2, 4])\n",
    "gpt2_layer = tx.modules.Embed(\n",
    "    num_embeddings=cfg.vocab_dim,\n",
    "    features=cfg.model_dim,\n",
    "    init_range=cfg.init_range,\n",
    ")\n",
    "load_gpt2_test(Embed, gpt2_layer, gpt2_params[\"embed\"], tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4)\n",
      "Output shape: (2, 768) \n",
      "\n",
      "Input shape: (45,)\n",
      "Output shape: (45, 768)\n",
      "Reference output shape: (45, 768) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    cfg: Config\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = self.param(\n",
    "            \"embedding\",\n",
    "            nn.initializers.normal(stddev=cfg.init_range),\n",
    "            (cfg.context_length, cfg.model_dim),\n",
    "        )\n",
    "\n",
    "    def __call__(self, tokens: Int[Array, \"seq\"]) -> Float[Array, \"seq model_dim\"]:\n",
    "        seq_len = tokens.shape[0]\n",
    "        return self.embedding[:seq_len]\n",
    "\n",
    "\n",
    "rand_int_test(PosEmbed, [2, 4])\n",
    "gpt2_layer = tx.modules.PosEmbed(\n",
    "    num_embeddings=cfg.context_length,\n",
    "    features=cfg.model_dim,\n",
    "    init_range=cfg.init_range,\n",
    ")\n",
    "load_gpt2_test(PosEmbed, gpt2_layer, gpt2_params[\"pos_embed\"], tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b805f1a24a146351cd8dedea7ba6f2d165b63b34e3029cd12d1a8f2ce65f719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from \n",
    "\n",
    "\n",
    "class FlaxGPT2Attention(nn.Module):\n",
    "    config: GPT2Config\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "    causal: bool = True\n",
    "    is_cross_attention: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        config = self.config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        if self.is_cross_attention:\n",
    "            self.c_attn = FlaxConv1D(2 * self.embed_dim, dtype=self.dtype)\n",
    "            self.q_attn = FlaxConv1D(self.embed_dim, dtype=self.dtype)\n",
    "        else:\n",
    "            self.c_attn = FlaxConv1D(3 * self.embed_dim, dtype=self.dtype)\n",
    "        self.c_proj = FlaxConv1D(self.embed_dim, dtype=self.dtype)\n",
    "\n",
    "        self.resid_dropout = nn.Dropout(rate=config.resid_pdrop)\n",
    "\n",
    "        if self.causal:\n",
    "            self.causal_mask = make_causal_mask(\n",
    "                jnp.ones((1, config.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\"\n",
    "            )\n",
    "\n",
    "    def _split_heads(self, hidden_states):\n",
    "        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n",
    "\n",
    "    def _merge_heads(self, hidden_states):\n",
    "        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n",
    "\n",
    "    @nn.compact\n",
    "    def _concatenate_to_cache(self, key, value, query, attention_mask):\n",
    "        \"\"\"\n",
    "        This function takes projected key, value states from a single input token and concatenates the states to cached\n",
    "        states from previous steps. This function is slighly adapted from the official Flax repository:\n",
    "        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n",
    "        \"\"\"\n",
    "        # detect if we're initializing by absence of existing cache data.\n",
    "        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n",
    "        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n",
    "        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n",
    "        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n",
    "\n",
    "        if is_initialized:\n",
    "            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n",
    "            # update key, value caches with our new 1d spatial slices\n",
    "            cur_index = cache_index.value\n",
    "            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n",
    "            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n",
    "            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n",
    "            cached_key.value = key\n",
    "            cached_value.value = value\n",
    "            num_updated_cache_vectors = query.shape[1]\n",
    "            cache_index.value = cache_index.value + num_updated_cache_vectors\n",
    "            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n",
    "            pad_mask = jnp.broadcast_to(\n",
    "                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n",
    "                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n",
    "            )\n",
    "            attention_mask = combine_masks(pad_mask, attention_mask)\n",
    "        return key, value, attention_mask\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        key_value_states: Optional[jnp.ndarray] = None,\n",
    "        attention_mask=None,\n",
    "        deterministic: bool = True,\n",
    "        init_cache: bool = False,\n",
    "        output_attentions: bool = False,\n",
    "    ):\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "        batch_size = hidden_states.shape[0]\n",
    "\n",
    "        if not is_cross_attention:\n",
    "            qkv_out = self.c_attn(hidden_states)\n",
    "            query, key, value = jnp.split(qkv_out, 3, axis=2)\n",
    "        else:\n",
    "            q_out = self.q_attn(hidden_states)\n",
    "            (query,) = jnp.split(q_out, 1, axis=2)\n",
    "            kv_out = self.c_attn(key_value_states)\n",
    "            key, value = jnp.split(kv_out, 2, axis=2)\n",
    "\n",
    "        query = self._split_heads(query)\n",
    "        key = self._split_heads(key)\n",
    "        value = self._split_heads(value)\n",
    "\n",
    "        query_length, key_length = query.shape[1], key.shape[1]\n",
    "\n",
    "        if self.causal:\n",
    "            if self.has_variable(\"cache\", \"cached_key\"):\n",
    "                mask_shift = self.variables[\"cache\"][\"cache_index\"]\n",
    "                max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n",
    "                causal_mask = lax.dynamic_slice(\n",
    "                    self.causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n",
    "                )\n",
    "            else:\n",
    "                causal_mask = self.causal_mask[:, :, :query_length, :key_length]\n",
    "            causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n",
    "\n",
    "        # combine masks if needed\n",
    "        if attention_mask is not None and self.causal:\n",
    "            attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n",
    "            attention_mask = combine_masks(attention_mask, causal_mask)\n",
    "        elif self.causal:\n",
    "            attention_mask = causal_mask\n",
    "        elif attention_mask is not None:\n",
    "            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n",
    "\n",
    "        dropout_rng = None\n",
    "        if not deterministic and self.config.attn_pdrop > 0.0:\n",
    "            dropout_rng = self.make_rng(\"dropout\")\n",
    "\n",
    "        # During fast autoregressive decoding, we feed one position at a time,\n",
    "        # and cache the keys and values step by step.\n",
    "        if self.causal and (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n",
    "            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n",
    "\n",
    "        # transform boolean mask into float mask\n",
    "        if attention_mask is not None:\n",
    "            attention_bias = lax.select(\n",
    "                attention_mask > 0,\n",
    "                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n",
    "                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n",
    "            )\n",
    "        else:\n",
    "            attention_bias = None\n",
    "\n",
    "        # usual dot product attention\n",
    "        attn_weights = dot_product_attention_weights(\n",
    "            query,\n",
    "            key,\n",
    "            bias=attention_bias,\n",
    "            dropout_rng=dropout_rng,\n",
    "            dropout_rate=self.config.attn_pdrop,\n",
    "            deterministic=deterministic,\n",
    "            dtype=self.dtype,\n",
    "            precision=None,\n",
    "        )\n",
    "\n",
    "        attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n",
    "        attn_output = self._merge_heads(attn_output)\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n",
    "\n",
    "        outputs = (attn_output, attn_weights) if output_attentions else (attn_output,)\n",
    "        return outputs\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

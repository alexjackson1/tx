{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jaxtyping import Array\n",
    "import jax.numpy as jnp\n",
    "from tx import TransformerWithHooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "2023-10-15 04:51:39.687942: W external/tsl/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 736.19MiB (rounded to 771948032)requested by op \n",
      "2023-10-15 04:51:39.688796: W external/tsl/tsl/framework/bfc_allocator.cc:497] ******____*********************************************************************_____________________\n",
      "2023-10-15 04:51:39.689034: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2717] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 771947904 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:         8B\n",
      "              constant allocation:         0B\n",
      "        maybe_live_out allocation:  294.47MiB\n",
      "     preallocated temp allocation:  736.19MiB\n",
      "  preallocated temp fragmentation:         0B (0.00%)\n",
      "                 total allocation:    1.01GiB\n",
      "              total fragmentation:  147.24MiB (14.29%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 294.47MiB\n",
      "\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/mul\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f64[768,50257]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 147.24MiB\n",
      "\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n",
      "\t\tXLA Label: custom-call\n",
      "\t\tShape: u32[38597376]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 147.24MiB\n",
      "\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n",
      "\t\tXLA Label: custom-call\n",
      "\t\tShape: u32[38597376]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 147.24MiB\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: u32[38597376]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 147.24MiB\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: u32[38597376]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 147.24MiB\n",
      "\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: u32[38597376]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 16B\n",
      "\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: (u32[38597376], u32[38597376])\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 16B\n",
      "\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n",
      "\t\tXLA Label: custom-call\n",
      "\t\tShape: (u32[38597376], u32[38597376])\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 16B\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: (u32[38597376], u32[38597376])\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 8B\n",
      "\t\tEntry Parameter Subshape: u32[2]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 771947904 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:         8B\n              constant allocation:         0B\n        maybe_live_out allocation:  294.47MiB\n     preallocated temp allocation:  736.19MiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:    1.01GiB\n              total fragmentation:  147.24MiB (14.29%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 294.47MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/mul\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: fusion\n\t\tShape: f64[768,50257]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 147.24MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: custom-call\n\t\tShape: u32[38597376]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 147.24MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: custom-call\n\t\tShape: u32[38597376]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 147.24MiB\n\t\tXLA Label: fusion\n\t\tShape: u32[38597376]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 147.24MiB\n\t\tXLA Label: fusion\n\t\tShape: u32[38597376]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 147.24MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: fusion\n\t\tShape: u32[38597376]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 16B\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: fusion\n\t\tShape: (u32[38597376], u32[38597376])\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 16B\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: custom-call\n\t\tShape: (u32[38597376], u32[38597376])\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 16B\n\t\tXLA Label: fusion\n\t\tShape: (u32[38597376], u32[38597376])\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 8B\n\t\tEntry Parameter Subshape: u32[2]\n\t\t==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reference_gpt2 \u001b[39m=\u001b[39m TransformerWithHooks\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mgpt2\u001b[39;49m\u001b[39m\"\u001b[39;49m, decode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/workspace/tx/tx/api/TransformerWithHooks.py:39\u001b[0m, in \u001b[0;36mTransformerWithHooks.from_pretrained\u001b[0;34m(cls, model_id, decode, dtype)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\n\u001b[1;32m     37\u001b[0m     \u001b[39mcls\u001b[39m, model_id: \u001b[39mstr\u001b[39m, decode: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, dtype: Optional[jnp\u001b[39m.\u001b[39mdtype] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTransformerWithHooks\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 39\u001b[0m     model \u001b[39m=\u001b[39m load_hf_model(model_id, decode\u001b[39m=\u001b[39;49mdecode, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(model)\n",
      "File \u001b[0;32m~/workspace/tx/tx/models/__init__.py:137\u001b[0m, in \u001b[0;36mload_hf_model\u001b[0;34m(model_name, decode, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     hf_config: HfGPT2Config \u001b[39m=\u001b[39m hf_config\n\u001b[1;32m    124\u001b[0m     config \u001b[39m=\u001b[39m GPT2Config(\n\u001b[1;32m    125\u001b[0m         decode\u001b[39m=\u001b[39mdecode,\n\u001b[1;32m    126\u001b[0m         model_dim\u001b[39m=\u001b[39mhf_config\u001b[39m.\u001b[39mn_embd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m         param_dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m    136\u001b[0m     )\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m GPT2TransformerModel(config, model_name, tokenizer_name\u001b[39m=\u001b[39;49mmodel_name)\n\u001b[1;32m    138\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00marchitecture\u001b[39m}\u001b[39;00m\u001b[39m is not currently supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/tx/tx/models/gpt2/__init__.py:24\u001b[0m, in \u001b[0;36mGPT2TransformerModel.__init__\u001b[0;34m(self, config, model_name, tokenizer_name, cache)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     18\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     19\u001b[0m     config: GPT2Config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     cache: Optional[PyTree[Array]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m ):\n\u001b[0;32m---> 24\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     25\u001b[0m         config,\n\u001b[1;32m     26\u001b[0m         GPT2Transformer\u001b[39m.\u001b[39;49mfrom_config(config),\n\u001b[1;32m     27\u001b[0m         AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(tokenizer_name),\n\u001b[1;32m     28\u001b[0m         GPT2Loader\u001b[39m.\u001b[39;49mload_params(model_name, config),\n\u001b[1;32m     29\u001b[0m         cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     30\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/tx/tx/models/__init__.py:71\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[0;34m(self, config, module, tokenizer, params, cache)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutable \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     70\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mdecode:\n\u001b[0;32m---> 71\u001b[0m     state \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49minit(\n\u001b[1;32m     72\u001b[0m         jr\u001b[39m.\u001b[39;49mPRNGKey(\u001b[39m0\u001b[39;49m), jnp\u001b[39m.\u001b[39;49mones((config\u001b[39m.\u001b[39;49mcontext_length,), jnp\u001b[39m.\u001b[39;49mint32)\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39m=\u001b[39m state[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutable \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/workspace/tx/tx/models/gpt2/modules.py:378\u001b[0m, in \u001b[0;36mGPT2Transformer.__call__\u001b[0;34m(self, tokens, hooks)\u001b[0m\n\u001b[1;32m    370\u001b[0m x \u001b[39m=\u001b[39m LayerNorm(\n\u001b[1;32m    371\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mln_f\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    372\u001b[0m     epsilon\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm_eps,\n\u001b[1;32m    373\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m    374\u001b[0m     param_dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_dtype,\n\u001b[1;32m    375\u001b[0m )(x, hooks)\n\u001b[1;32m    376\u001b[0m x \u001b[39m=\u001b[39m apply_hooks((\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath, \u001b[39m\"\u001b[39m\u001b[39moutput_hook\u001b[39m\u001b[39m\"\u001b[39m), hooks, x, module\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 378\u001b[0m logits \u001b[39m=\u001b[39m Unembed(\n\u001b[1;32m    379\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39munembed\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    380\u001b[0m     features\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_dim,\n\u001b[1;32m    381\u001b[0m     num_embeddings\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab_dim,\n\u001b[1;32m    382\u001b[0m     init_range\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_range,\n\u001b[1;32m    383\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    384\u001b[0m     param_dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_dtype,\n\u001b[1;32m    385\u001b[0m )(x)\n\u001b[1;32m    387\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/workspace/tx/tx/modules/common.py:203\u001b[0m, in \u001b[0;36mUnembed.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    200\u001b[0m dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39mor\u001b[39;00m jnp\u001b[39m.\u001b[39mresult_type(x)\n\u001b[1;32m    201\u001b[0m x \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39masarray(x, dtype)\n\u001b[0;32m--> 203\u001b[0m kernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam(\n\u001b[1;32m    204\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mkernel\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    205\u001b[0m     nn\u001b[39m.\u001b[39;49minitializers\u001b[39m.\u001b[39;49mnormal(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_range),\n\u001b[1;32m    206\u001b[0m     (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_embeddings),\n\u001b[1;32m    207\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_dtype,\n\u001b[1;32m    208\u001b[0m )\n\u001b[1;32m    209\u001b[0m bias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam(\n\u001b[1;32m    210\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    211\u001b[0m     jax\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39minitializers\u001b[39m.\u001b[39mzeros,\n\u001b[1;32m    212\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_embeddings,),\n\u001b[1;32m    213\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_dtype,\n\u001b[1;32m    214\u001b[0m )\n\u001b[1;32m    215\u001b[0m \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39m...sf,fv->...sv\u001b[39m\u001b[39m\"\u001b[39m, x, kernel) \u001b[39m+\u001b[39m bias\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/nn/initializers.py:155\u001b[0m, in \u001b[0;36mnormal.<locals>.init\u001b[0;34m(key, shape, dtype)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit\u001b[39m(key: KeyArray,\n\u001b[1;32m    152\u001b[0m          shape: core\u001b[39m.\u001b[39mShape,\n\u001b[1;32m    153\u001b[0m          dtype: DTypeLikeInexact \u001b[39m=\u001b[39m dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m    154\u001b[0m   dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mcanonicalize_dtype(dtype)\n\u001b[0;32m--> 155\u001b[0m   \u001b[39mreturn\u001b[39;00m random\u001b[39m.\u001b[39;49mnormal(key, shape, dtype) \u001b[39m*\u001b[39m stddev\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/random.py:730\u001b[0m, in \u001b[0;36mnormal\u001b[0;34m(key, shape, dtype)\u001b[0m\n\u001b[1;32m    728\u001b[0m dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mcanonicalize_dtype(dtype)\n\u001b[1;32m    729\u001b[0m shape \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39mas_named_shape(shape)\n\u001b[0;32m--> 730\u001b[0m \u001b[39mreturn\u001b[39;00m _normal(key, shape, dtype)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/workspace/tx/env/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1152\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxla_executable\u001b[39m.\u001b[39;49mexecute_sharded(input_bufs)\n\u001b[1;32m   1153\u001b[0m \u001b[39mif\u001b[39;00m dispatch\u001b[39m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1154\u001b[0m   out_arrays \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 771947904 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:         8B\n              constant allocation:         0B\n        maybe_live_out allocation:  294.47MiB\n     preallocated temp allocation:  736.19MiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:    1.01GiB\n              total fragmentation:  147.24MiB (14.29%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 294.47MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/mul\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: fusion\n\t\tShape: f64[768,50257]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 147.24MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: custom-call\n\t\tShape: u32[38597376]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 147.24MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: custom-call\n\t\tShape: u32[38597376]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 147.24MiB\n\t\tXLA Label: fusion\n\t\tShape: u32[38597376]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 147.24MiB\n\t\tXLA Label: fusion\n\t\tShape: u32[38597376]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 147.24MiB\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: fusion\n\t\tShape: u32[38597376]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 16B\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: fusion\n\t\tShape: (u32[38597376], u32[38597376])\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 16B\n\t\tOperator: op_name=\"jit(_normal)/jit(main)/jit(_normal_real)/jit(_uniform)/threefry2x32\" source_file=\"/home/alexj/workspace/tx/env/lib/python3.11/site-packages/flax/core/scope.py\" source_line=983\n\t\tXLA Label: custom-call\n\t\tShape: (u32[38597376], u32[38597376])\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 16B\n\t\tXLA Label: fusion\n\t\tShape: (u32[38597376], u32[38597376])\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 8B\n\t\tEntry Parameter Subshape: u32[2]\n\t\t==========================\n\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = TransformerWithHooks.from_pretrained(\"gpt2\", decode=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256 15496    11   314   716]\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"Hello, I am\"\n",
    "tokens: Array = reference_gpt2.to_tokens(reference_text, prepend_bos=True)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reference_gpt2.to_str(tokens), end=\"\", flush=True)\n",
    "# for i in range(tokens.shape[0]):\n",
    "#     # Pass sequence through the model to get new output\n",
    "#     logits, _ = reference_gpt2(tokens[: i + 1])\n",
    "#     # Get the predicted token at the end of our sequence\n",
    "#     next_token = jnp.argmax(logits, axis=-1)[-1]\n",
    "#     # Decode and print the result\n",
    "#     next_char = reference_gpt2.to_str(next_token)\n",
    "#     print(f\"next_token[{i}]: {next_char}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a\n"
     ]
    }
   ],
   "source": [
    "logits, _ = reference_gpt2(tokens)\n",
    "next_token = jnp.argmax(jax.nn.softmax(logits), axis=-1)[-1]\n",
    "next_char = reference_gpt2.to_str(next_token)\n",
    "print(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>Hello, I am a student at the University of California, Berkeley."
     ]
    }
   ],
   "source": [
    "cur_tokens = tokens\n",
    "print(reference_gpt2.to_str(cur_tokens), end=\"\", flush=True)\n",
    "for i in range(10):\n",
    "    # Pass sequence through the model to get new output\n",
    "    logits, _ = reference_gpt2(cur_tokens)\n",
    "    # Get the predicted token at the end of our sequence\n",
    "    next_token = jnp.argmax(logits, axis=-1)[-1]\n",
    "    # Decode and print the result\n",
    "    next_char = reference_gpt2.to_str(next_token)\n",
    "    print(next_char, end=\"\", flush=True)\n",
    "    # Define new input sequence, by appending the previously generated token\n",
    "    cur_tokens = jnp.append(cur_tokens, next_token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b805f1a24a146351cd8dedea7ba6f2d165b63b34e3029cd12d1a8f2ce65f719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

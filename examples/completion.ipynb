{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# notebook\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "from tx.models.gpt2 import PretrainedGPT2Model\n",
    "from tx.hooks import HookMap, HookPoint, Hook\n",
    "from tx.network import GenerativeModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PretrainedGPT2Model.tx_config\n",
    "config.decode = True\n",
    "\n",
    "\n",
    "def store_hook(x, module: nn.Module, hook_point: HookPoint):\n",
    "    module.sow(\"intermediates\", hook_point.value, x)\n",
    "    return x\n",
    "\n",
    "\n",
    "reference_gpt2 = GenerativeModel(\n",
    "    config=config,\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained(\"gpt2\"),\n",
    "    params=PretrainedGPT2Model.from_pretrained(\"gpt2\").to_params(),\n",
    "    hooks=HookMap(embed=Hook(store_hook)),\n",
    "    hook_collections=[\"intermediates\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256    40   716   281  4998  1960   382 19741    11   875 12342    12\n",
      "  8807    11   402 11571    12    17  3918 47385    13  1881  1110   314\n",
      "   481  7074  1692  1241  4430   290  1011   625   262   995     0]\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text, prepend_bos=True)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current <|endoftext|> Next \n",
      "\n",
      "Current I Next .\n",
      "Current  am Next .\n",
      "Current  an Next  the\n",
      "Current  amazing Next ,\n",
      "Current  aut Next .\n",
      "Current ore Next ,\n",
      "Current gressive Next ,\n",
      "Current , Next  the\n",
      "Current  dec Next .\n",
      "Current oder Next .\n",
      "Current - Next \n",
      "\n",
      "Current only Next ,\n",
      "Current , Next  the\n",
      "Current  G Next .\n",
      "Current PT Next .\n",
      "Current - Next \n",
      "\n",
      "Current 2 Next .\n",
      "Current  style Next .\n",
      "Current  transformer Next .\n",
      "Current . Next \n",
      "\n",
      "Current  One Next  of\n",
      "Current  day Next ,\n",
      "Current  I Next .\n",
      "Current  will Next ,\n",
      "Current  exceed Next .\n",
      "Current  human Next ,\n",
      "Current  level Next .\n",
      "Current  intelligence Next ,\n",
      "Current  and Next  the\n",
      "Current  take Next  the\n",
      "Current  over Next  the\n",
      "Current  the Next  the\n",
      "Current  world Next .\n",
      "Current ! Next \n",
      "\n",
      "Current \n",
      " Next \n",
      "\n",
      "Current \n",
      " Next \n",
      "\n",
      "Current \n",
      " Next \n",
      "\n",
      "Current \n",
      " Next \n",
      "\n",
      "Current \n",
      " Next \n",
      "\n",
      "Current \n",
      " Next \n",
      "\n",
      "Current \n",
      " Next \n",
      "\n",
      "Current \n",
      " Next \n",
      "\n",
      "Current \n",
      " Next \n",
      "\n",
      "Current \n",
      " Next \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(reference_gpt2.to_str(tokens), end=\"\", flush=True)\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    token = tokens[None, i]\n",
    "    # print(reference_gpt2.to_str(token), end=\"\", flush=True)\n",
    "    logits, _ = reference_gpt2(token)\n",
    "    next_token = jnp.argmax(logits[-1], axis=-1, keepdims=True)\n",
    "    print(\n",
    "        \"Current\",\n",
    "        reference_gpt2.to_str(token),\n",
    "        \"Next\",\n",
    "        reference_gpt2.to_str(next_token),\n",
    "    )\n",
    "\n",
    "new_tokens = jnp.concatenate([tokens, next_token], axis=-1)\n",
    "\n",
    "for i in range(10):\n",
    "    # Pass sequence through the model to get new output\n",
    "    logits, _ = reference_gpt2(new_tokens[None, -1])\n",
    "    # Get the predicted token at the end of our sequence\n",
    "    next_token = jnp.argmax(logits[-1], axis=-1, keepdims=True)\n",
    "    # Decode and print the result\n",
    "    # print(reference_gpt2.to_str(next_token), end=\"\", flush=True)\n",
    "    print(\n",
    "        \"Current\",\n",
    "        reference_gpt2.to_str(new_tokens[None, -1]),\n",
    "        \"Next\",\n",
    "        reference_gpt2.to_str(next_token),\n",
    "    )\n",
    "    # Define new input sequence, by appending the previously generated token\n",
    "    prompt = jnp.concatenate([new_tokens, next_token], axis=-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b805f1a24a146351cd8dedea7ba6f2d165b63b34e3029cd12d1a8f2ce65f719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

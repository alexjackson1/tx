{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# notebook\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "from tx.models.gpt2 import PretrainedGPT2Model\n",
    "from tx.hooks import HookMap, HookPoint, Hook\n",
    "from tx.network import GenerativeModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PretrainedGPT2Model.tx_config\n",
    "config.decode = True\n",
    "\n",
    "\n",
    "def store_hook(x, module: nn.Module, hook_point: HookPoint):\n",
    "    module.sow(\"intermediates\", hook_point.value, x)\n",
    "    return x\n",
    "\n",
    "\n",
    "reference_gpt2 = GenerativeModel(\n",
    "    config=config,\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained(\"gpt2\"),\n",
    "    params=PretrainedGPT2Model.from_pretrained(\"gpt2\").to_params(),\n",
    "    hooks=HookMap(embed=Hook(store_hook)),\n",
    "    hook_collections=[\"intermediates\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256    40   716   281  4998  1960   382 19741    11   875 12342    12\n",
      "  8807    11   402 11571    12    17  3918 47385    13  1881  1110   314\n",
      "   481  7074  1692  1241  4430   290  1011   625   262   995     0]\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text, prepend_bos=True)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <|endoftext|> [\n",
      "]\n",
      "1: I [.]\n",
      "2:  am [.]\n",
      "3:  an [ the]\n",
      "4:  amazing [,]\n",
      "5:  aut [.]\n",
      "6: ore [,]\n",
      "7: gressive [,]\n",
      "8: , [ the]\n",
      "9:  dec [.]\n",
      "10: oder [.]\n",
      "11: - [\n",
      "]\n",
      "12: only [,]\n",
      "13: , [ the]\n",
      "14:  G [.]\n",
      "15: PT [.]\n",
      "16: - [\n",
      "]\n",
      "17: 2 [.]\n",
      "18:  style [.]\n",
      "19:  transformer [.]\n",
      "20: . [\n",
      "]\n",
      "21:  One [ of]\n",
      "22:  day [,]\n",
      "23:  I [.]\n",
      "24:  will [,]\n",
      "25:  exceed [.]\n",
      "26:  human [,]\n",
      "27:  level [.]\n",
      "28:  intelligence [,]\n",
      "29:  and [ the]\n",
      "30:  take [ the]\n",
      "31:  over [ the]\n",
      "32:  the [ the]\n",
      "33:  world [.]\n",
      "34: ! [\n",
      "]\n",
      " 0: \n",
      " [\n",
      "]\n",
      " 1: \n",
      " [\n",
      "]\n",
      " 2: \n",
      " [\n",
      "]\n",
      " 3: \n",
      " [\n",
      "]\n",
      " 4: \n",
      " [\n",
      "]\n",
      " 5: \n",
      " [\n",
      "]\n",
      " 6: \n",
      " [\n",
      "]\n",
      " 7: \n",
      " [\n",
      "]\n",
      " 8: \n",
      " [\n",
      "]\n",
      " 9: \n",
      " [\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# print(reference_gpt2.to_str(tokens), end=\"\", flush=True)\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    token = tokens[None, i]\n",
    "    # print(reference_gpt2.to_str(token), end=\"\", flush=True)\n",
    "    logits, _ = reference_gpt2(token)\n",
    "    next_token = jnp.argmax(logits, axis=-1)\n",
    "    cur_char, next_char = tuple(map(reference_gpt2.to_str, [token, next_token]))\n",
    "    print(f\"{i}: {cur_char} [{next_char}]\")\n",
    "\n",
    "new_tokens = jnp.concatenate([tokens, next_token], axis=-1)\n",
    "\n",
    "for i in range(10):\n",
    "    token = new_tokens[None, -1]\n",
    "    # Pass sequence through the model to get new output\n",
    "    logits, _ = reference_gpt2(token)\n",
    "    # Get the predicted token at the end of our sequence\n",
    "    next_token = jnp.argmax(logits, axis=-1)\n",
    "    # Decode and print the result\n",
    "    # print(reference_gpt2.to_str(next_token), end=\"\", flush=True)\n",
    "    cur_char, next_char = tuple(map(reference_gpt2.to_str, [token, next_token]))\n",
    "    print(f\"{i:2}: {cur_char} [{next_char}]\")\n",
    "    # Define new input sequence, by appending the previously generated token\n",
    "    prompt = jnp.concatenate([new_tokens, next_token], axis=-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b805f1a24a146351cd8dedea7ba6f2d165b63b34e3029cd12d1a8f2ce65f719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "import functools\n",
    "from typing import Callable, Optional, Tuple, TypeVar\n",
    "from jaxtyping import Float, Array, PRNGKeyArray, Bool\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import flax.linen as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtype = TypeVar(\"Dtype\", bound=jnp.dtype)\n",
    "Shape = Tuple[int, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tx.models.gpt2 import PretrainedGPT2Model\n",
    "from tx.network import GenerativeModel\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "\n",
    "reference_gpt2 = GenerativeModel(\n",
    "    config=PretrainedGPT2Model.tx_config,\n",
    "    variables={\"params\": PretrainedGPT2Model.from_pretrained(\"gpt2\").to_params()},\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained(\"gpt2\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitFn = Callable[[PRNGKeyArray, Shape, Dtype], Array]\n",
    "\n",
    "\n",
    "class FlaxAttentionImplementation(nn.Module):\n",
    "    num_heads: int\n",
    "    qkv_features: Optional[int] = None\n",
    "    out_features: Optional[int] = None\n",
    "    kernel_init: InitFn = nn.initializers.xavier_uniform()\n",
    "    bias_init: InitFn = nn.initializers.zeros_init()\n",
    "    use_bias: bool = True\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        inputs_q: Float[Array, \"... QL\"],  # QL = query length\n",
    "        mask: Optional[Array] = None,\n",
    "    ):\n",
    "        features = self.out_features or inputs_q.shape[-1]\n",
    "        qkv_features = self.qkv_features or inputs_q.shape[-1]\n",
    "        head_dim = qkv_features // self.num_heads\n",
    "        # inputs_kv = inputs_kv if inputs_kv is not None else inputs_q\n",
    "\n",
    "        dense = functools.partial(\n",
    "            nn.linear.DenseGeneral,\n",
    "            axis=-1,\n",
    "            features=(self.num_heads, head_dim),\n",
    "            kernel_init=self.kernel_init,\n",
    "            bias_init=self.bias_init,\n",
    "            use_bias=self.use_bias,\n",
    "        )\n",
    "\n",
    "        # project inputs_q to multi-headed q/k/v\n",
    "        # dimensions are then [batch..., length, n_heads, n_features_per_head]\n",
    "\n",
    "        query, key, value = (\n",
    "            dense(name=\"query\")(inputs_q),\n",
    "            dense(name=\"key\")(inputs_q),\n",
    "            dense(name=\"value\")(inputs_q),\n",
    "        )\n",
    "\n",
    "        # calculate attention matrix\n",
    "        depth = query.shape[-1]\n",
    "        query = query / jnp.sqrt(depth)\n",
    "\n",
    "        # attn weight shape is (batch..., num_heads, q_length, kv_length)\n",
    "        attn_scores = jnp.einsum(\"...qhd,...khd->...hqk\", query, key)\n",
    "\n",
    "        # apply attention mask\n",
    "        if mask is not None:\n",
    "            big_neg = jnp.finfo(jnp.float32).min\n",
    "            attn_scores = jnp.where(mask, attn_scores, big_neg)\n",
    "\n",
    "        # normalize the attention weights\n",
    "        attn_weights = jax.nn.softmax(attn_scores)\n",
    "\n",
    "        # return weighted sum over values for each query position\n",
    "        z = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n",
    "\n",
    "        # back to the original inputs dimensions\n",
    "        out = nn.linear.DenseGeneral(\n",
    "            features=features,\n",
    "            axis=(-2, -1),\n",
    "            kernel_init=self.kernel_init,\n",
    "            bias_init=self.bias_init,\n",
    "            use_bias=self.use_bias,\n",
    "            name=\"out\",  # type: ignore[call-arg]\n",
    "        )(z)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from jaxtyping import Array, Float\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import flax.struct as struct\n",
    "\n",
    "\n",
    "class TXAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    head_dim: int\n",
    "    model_dim: int\n",
    "    init_range: float = 0.02\n",
    "    use_bias: bool = True\n",
    "\n",
    "    intermediates: List[str] = struct.field(default_factory=list)\n",
    "\n",
    "    def intermediate(self, name: str, value: Array) -> bool:\n",
    "        if name in self.intermediates:\n",
    "            return self.sow(\"intermediates\", name, value)\n",
    "        return False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Float[Array, \"seq embed\"]) -> Float[Array, \"seq embed\"]:\n",
    "        \"\"\"\n",
    "        References:\n",
    "        - `flax.linen.attention`.\n",
    "        \"\"\"\n",
    "        init_dense = partial(\n",
    "            nn.DenseGeneral,\n",
    "            kernel_init=jax.nn.initializers.normal(stddev=self.init_range),\n",
    "            bias_init=jax.nn.initializers.zeros,\n",
    "            use_bias=self.use_bias,\n",
    "        )\n",
    "\n",
    "        # Apply a linear transformation to the input tensor.\n",
    "        hidden_states = init_dense(name=\"c_attn\", features=3 * self.model_dim)(x)\n",
    "\n",
    "        # Split the hidden states into query, key, and value.\n",
    "        query, key, value = self._split_outputs(hidden_states)\n",
    "        query_length, key_length = query.shape[-3], key.shape[-3]\n",
    "        print(query.shape, key.shape, value.shape)\n",
    "        self._qkv_intermediates((query, key, value))\n",
    "\n",
    "        # Compute the attention weights.\n",
    "        query = query / jnp.sqrt(query.shape[-1])\n",
    "        scores = jnp.einsum(\"...qhd,...khd->...hqk\", query, key)\n",
    "        self.intermediate(\"scores\", scores)\n",
    "\n",
    "        # Apply the causal mask to the attention weights.\n",
    "        mask = nn.make_causal_mask(jnp.ones((x.shape[0],), dtype=\"bool\"))[\n",
    "            :, :query_length, :key_length\n",
    "        ]\n",
    "        big_neg = jnp.finfo(jnp.float32).min\n",
    "        scores = jnp.where(mask, scores, big_neg)\n",
    "\n",
    "        # Normalize the attention weights\n",
    "        weights = jax.nn.softmax(scores)\n",
    "        self.intermediate(\"weights\", weights)\n",
    "\n",
    "        # Apply the attention pattern to the value tensor.\n",
    "        z = jnp.einsum(\"...hqk,...khd->...qhd\", weights, value)\n",
    "        self.intermediate(\"z\", z)\n",
    "\n",
    "        # Apply a linear transformation to the attention output.\n",
    "        merged_z = self._merge_heads(z)\n",
    "        output = init_dense(name=\"c_proj\", features=self.model_dim)(merged_z)\n",
    "        return output\n",
    "\n",
    "    def _qkv_intermediates(self, qkv: Tuple[Array, Array, Array]) -> bool:\n",
    "        ret_vals = []\n",
    "        for name, value in zip((\"query\", \"key\", \"value\"), qkv):\n",
    "            ret_vals.append(self.intermediate(name, value))\n",
    "\n",
    "        return all(ret_vals)\n",
    "\n",
    "    def _split_outputs(self, states: Array):\n",
    "        return map(self._split_heads, jnp.split(states, 3, axis=-1))\n",
    "\n",
    "    def _split_heads(self, states: Array):\n",
    "        return states.reshape((states.shape[0], self.num_heads, self.head_dim))\n",
    "\n",
    "    def _merge_heads(self, states: Array):\n",
    "        return states.reshape((states.shape[0], self.model_dim))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Attention Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3, 12) (4, 3, 12) (4, 3, 12)\n",
      "(4, 3, 12) (4, 3, 12) (4, 3, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_HEADS = 4\n",
    "HEAD_DIM = 10\n",
    "MODEL_DIM = NUM_HEADS * HEAD_DIM\n",
    "\n",
    "flax_attn = nn.MultiHeadDotProductAttention(\n",
    "    num_heads=NUM_HEADS, qkv_features=MODEL_DIM, out_features=MODEL_DIM\n",
    ")\n",
    "tx_attn = TXAttention(num_heads=NUM_HEADS, head_dim=HEAD_DIM, model_dim=MODEL_DIM)\n",
    "\n",
    "\n",
    "def convert_params(tx_params):\n",
    "    c_attn, c_proj = tx_params[\"c_attn\"], tx_params[\"c_proj\"]\n",
    "\n",
    "    qkv_kernel = jnp.split(c_attn[\"kernel\"], 3, axis=-1)\n",
    "    reshape_kernel = lambda a: jnp.reshape(a, (qkv_kernel[0].shape[0], NUM_HEADS, HEAD_DIM))\n",
    "    q_kernel, k_kernel, v_kernel = tuple(map(reshape_kernel, qkv_kernel))\n",
    "    o_kernel = jnp.reshape(c_proj[\"kernel\"], (NUM_HEADS, HEAD_DIM, MODEL_DIM))\n",
    "\n",
    "    qkv_bias = jnp.split(c_attn[\"bias\"], 3, axis=-1)\n",
    "    reshape_bias = lambda a: jnp.reshape(a, (NUM_HEADS, HEAD_DIM))\n",
    "    q_bias, k_bias, v_bias = tuple(map(reshape_bias, qkv_bias))\n",
    "    o_bias = c_proj[\"bias\"]\n",
    "\n",
    "    flax_params = {}\n",
    "    flax_params[\"query\"] = {\"kernel\": q_kernel, \"bias\": q_bias}\n",
    "    flax_params[\"key\"] = {\"kernel\": k_kernel, \"bias\": k_bias}\n",
    "    flax_params[\"value\"] = {\"kernel\": v_kernel, \"bias\": v_bias}\n",
    "    flax_params[\"out\"] = {\"kernel\": o_kernel, \"bias\": o_bias}\n",
    "\n",
    "    return flax_params\n",
    "\n",
    "\n",
    "ex_inputs = jnp.array([[1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1]])\n",
    "tx_params = tx_attn.init(jr.PRNGKey(0), x=ex_inputs)[\"params\"]\n",
    "flax_params = convert_params(tx_params)\n",
    "\n",
    "tx_out = tx_attn.apply({\"params\": tx_params}, ex_inputs)\n",
    "flax_out = flax_attn.apply(\n",
    "    {\"params\": flax_params},\n",
    "    ex_inputs,\n",
    "    ex_inputs,\n",
    "    mask=nn.make_causal_mask(\n",
    "        jnp.ones((ex_inputs.shape[0],), dtype=\"bool\"),\n",
    "        dtype=\"bool\",\n",
    "    )[:, :4, :4],\n",
    ")\n",
    "\n",
    "jnp.allclose(tx_out, flax_out, atol=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': {'kernel': (3, 2, 4)},\n",
       " 'out': {'kernel': (2, 4, 8)},\n",
       " 'query': {'kernel': (3, 2, 4)},\n",
       " 'value': {'kernel': (3, 2, 4)}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "jax.tree_util.tree_map(lambda a: a.shape, tx_as_flax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 3.5190952e-04, -3.6897347e-04,  3.5196170e-04, -2.6260878e-04,\n",
       "        -2.1131843e-04,  3.2719318e-04, -5.1394856e-04,  5.3117721e-04],\n",
       "       [ 3.9654266e-04, -1.9495170e-04,  3.9853103e-04, -5.1042713e-05,\n",
       "        -6.5240380e-04,  2.3290190e-04, -4.9521885e-04,  4.6091323e-04],\n",
       "       [ 5.2868749e-04, -2.5989593e-04,  5.3130480e-04, -6.8043766e-05,\n",
       "        -8.6979882e-04,  3.1042617e-04, -6.6020573e-04,  6.1448303e-04],\n",
       "       [ 2.9878583e-04,  2.6193861e-04,  3.0228859e-04,  8.6668959e-05,\n",
       "        -5.8025989e-04,  3.4064267e-04, -4.2032293e-04,  6.4078835e-04]],      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_output = tx_attn.apply({\"params\": tx_params}, ex_inputs)\n",
    "tx_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.18964374,  0.16998684,  0.00492778, -0.39047906,  0.12759988,\n",
       "        -0.03431374,  0.2705934 ,  0.649871  ],\n",
       "       [-0.15222657,  0.15960044,  0.0206019 , -0.3847821 ,  0.09544811,\n",
       "        -0.04913256,  0.21884091,  0.64352965],\n",
       "       [-0.17099951,  0.15805584,  0.01345147, -0.3938407 ,  0.10670467,\n",
       "        -0.04940178,  0.24964438,  0.65235615],\n",
       "       [-0.18512593,  0.17309962, -0.02557041, -0.4040893 ,  0.14441134,\n",
       "        -0.01572113,  0.27858272,  0.6612222 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flax_output = flax_attn.apply({\"params\": flax_params}, ex_inputs)\n",
    "flax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 2.9841467e-04,  2.6236032e-04,  3.0198149e-04,  8.6865257e-05,\n",
       "        -5.7999179e-04,  3.4076214e-04, -4.2007797e-04,  6.4062094e-04],\n",
       "       [ 2.9800236e-04,  2.6269123e-04,  3.0171062e-04,  8.7038155e-05,\n",
       "        -5.7970901e-04,  3.4104238e-04, -4.1987709e-04,  6.4042286e-04],\n",
       "       [ 2.9793353e-04,  2.6288204e-04,  3.0160198e-04,  8.7080378e-05,\n",
       "        -5.7957176e-04,  3.4096732e-04, -4.1976344e-04,  6.4043072e-04],\n",
       "       [ 2.9878583e-04,  2.6193861e-04,  3.0228859e-04,  8.6668959e-05,\n",
       "        -5.8025989e-04,  3.4064267e-04, -4.2032293e-04,  6.4078835e-04]],      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_as_flax_output = flax_attn.apply({\"params\": tx_as_flax}, ex_inputs)\n",
    "tx_as_flax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_params = reference_gpt2.variables[\"params\"][\"block_0\"][\"attn\"]\n",
    "\n",
    "\n",
    "# def transform_params(params):\n",
    "#     attn_kernel, attn_bias = params[\"c_attn\"][\"kernel\"], params[\"c_attn\"][\"bias\"]\n",
    "#     proj_kernel, proj_bias = params[\"c_proj\"][\"kernel\"], params[\"c_proj\"][\"bias\"]\n",
    "\n",
    "#     def transform_kernel(a):\n",
    "#         a = jnp.transpose(a)\n",
    "#         a = jnp.reshape(a, (768, 12, 64))\n",
    "#         return a\n",
    "\n",
    "#     def transform_bias(b):\n",
    "#         return jnp.reshape(b, (12, 64))\n",
    "\n",
    "#     q_kernel, k_kernel, v_kernel = map(\n",
    "#         transform_kernel, jnp.split(attn_kernel, 3, axis=-1)\n",
    "#     )\n",
    "#     q_bias, k_bias, v_bias = map(transform_bias, jnp.split(attn_bias, 3, axis=-1))\n",
    "\n",
    "#     out_kernel = transform_kernel(proj_kernel)\n",
    "#     out_kernel = jnp.transpose(out_kernel, (1, 2, 0))\n",
    "#     # out_bias = jnp.reshape(proj_bias, (12, 64))\n",
    "\n",
    "#     # return {\n",
    "#     #     \"query\": {\"kernel\": q_kernel, \"bias\": q_bias},\n",
    "#     #     \"key\": {\"kernel\": k_kernel, \"bias\": k_bias},\n",
    "#     #     \"value\": {\"kernel\": v_kernel, \"bias\": v_bias},\n",
    "#     #     \"out\": {\"kernel\": out_kernel, \"bias\": proj_bias},\n",
    "#     # }\n",
    "#     return {\n",
    "#         \"query\": {\"kernel\": jnp.ones_like(q_kernel), \"bias\": jnp.zeros_like(q_bias)},\n",
    "#         \"key\": {\"kernel\": jnp.ones_like(k_kernel), \"bias\": jnp.zeros_like(k_bias)},\n",
    "#         \"value\": {\"kernel\": jnp.ones_like(v_kernel), \"bias\": jnp.zeros_like(v_bias)},\n",
    "#         \"out\": {\"kernel\": jnp.ones_like(out_kernel), \"bias\": jnp.zeros_like(proj_bias)},\n",
    "#     }\n",
    "\n",
    "\n",
    "# def block_params(params):\n",
    "#     # return {\n",
    "#     #     \"c_attn\": {\n",
    "#     #         \"kernel\": params[\"c_attn\"][\"kernel\"],\n",
    "#     #         \"bias\": params[\"c_attn\"][\"kernel\"],\n",
    "#     #     },\n",
    "#     #     \"c_proj\": {\n",
    "#     #         \"kernel\": params[\"c_proj\"][\"kernel\"],\n",
    "#     #         \"bias\": params[\"c_proj\"][\"bias\"],\n",
    "#     #     },\n",
    "#     # }\n",
    "#     return {\n",
    "#         \"query\": {\n",
    "#             \"kernel\": jnp.ones((768, 12, 64)),\n",
    "#             \"bias\": jnp.zeros((12, 64)),\n",
    "#         },\n",
    "#         \"key\": {\n",
    "#             \"kernel\": jnp.ones((768, 12, 64)),\n",
    "#             \"bias\": jnp.zeros((12, 64)),\n",
    "#         },\n",
    "#         \"value\": {\n",
    "#             \"kernel\": jnp.ones((768, 12, 64)),\n",
    "#             \"bias\": jnp.zeros((12, 64)),\n",
    "#         },\n",
    "#         \"proj\": {\n",
    "#             \"kernel\": jnp.ones_like(params[\"c_proj\"][\"kernel\"]),\n",
    "#             \"bias\": jnp.zeros_like(params[\"c_proj\"][\"bias\"]),\n",
    "#         },\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flax_variables = {\"params\": transform_params(test_params)}\n",
    "# flax_attn.apply(flax_variables, ex_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tx_attn.apply({\"params\": block_params(test_params)}, ex_inputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b805f1a24a146351cd8dedea7ba6f2d165b63b34e3029cd12d1a8f2ce65f719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax.linen import MultiHeadDotProductAttention as FlaxAttention\n",
    "from tx.modules import Attention as TxAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tx.models.gpt2 import PretrainedGPT2Model\n",
    "from tx.network import GenerativeModel\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "\n",
    "reference_gpt2 = GenerativeModel(\n",
    "    config=PretrainedGPT2Model.tx_config,\n",
    "    variables={\"params\": PretrainedGPT2Model.from_pretrained(\"gpt2\").to_params()},\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained(\"gpt2\"),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Attention Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HEADS = 4\n",
    "HEAD_DIM = 10\n",
    "MODEL_DIM = NUM_HEADS * HEAD_DIM\n",
    "\n",
    "flax_attn = nn.MultiHeadDotProductAttention(\n",
    "    num_heads=NUM_HEADS, qkv_features=MODEL_DIM, out_features=MODEL_DIM\n",
    ")\n",
    "tx_attn = TXAttention(num_heads=NUM_HEADS, head_dim=HEAD_DIM, model_dim=MODEL_DIM)\n",
    "\n",
    "\n",
    "def convert_params(tx_params):\n",
    "    c_attn, c_proj = tx_params[\"c_attn\"], tx_params[\"c_proj\"]\n",
    "\n",
    "    qkv_kernel = jnp.split(c_attn[\"kernel\"], 3, axis=-1)\n",
    "    reshape_kernel = lambda a: jnp.reshape(a, (qkv_kernel[0].shape[0], NUM_HEADS, HEAD_DIM))\n",
    "    q_kernel, k_kernel, v_kernel = tuple(map(reshape_kernel, qkv_kernel))\n",
    "    o_kernel = jnp.reshape(c_proj[\"kernel\"], (NUM_HEADS, HEAD_DIM, MODEL_DIM))\n",
    "\n",
    "    qkv_bias = jnp.split(c_attn[\"bias\"], 3, axis=-1)\n",
    "    reshape_bias = lambda a: jnp.reshape(a, (NUM_HEADS, HEAD_DIM))\n",
    "    q_bias, k_bias, v_bias = tuple(map(reshape_bias, qkv_bias))\n",
    "    o_bias = c_proj[\"bias\"]\n",
    "\n",
    "    flax_params = {}\n",
    "    flax_params[\"query\"] = {\"kernel\": q_kernel, \"bias\": q_bias}\n",
    "    flax_params[\"key\"] = {\"kernel\": k_kernel, \"bias\": k_bias}\n",
    "    flax_params[\"value\"] = {\"kernel\": v_kernel, \"bias\": v_bias}\n",
    "    flax_params[\"out\"] = {\"kernel\": o_kernel, \"bias\": o_bias}\n",
    "\n",
    "    return flax_params\n",
    "\n",
    "\n",
    "ex_inputs = jnp.array([[1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1]])\n",
    "tx_params = tx_attn.init(jr.PRNGKey(0), x=ex_inputs)[\"params\"]\n",
    "flax_params = convert_params(tx_params)\n",
    "\n",
    "tx_out = tx_attn.apply({\"params\": tx_params}, ex_inputs)\n",
    "flax_out = flax_attn.apply(\n",
    "    {\"params\": flax_params},\n",
    "    ex_inputs,\n",
    "    ex_inputs,\n",
    "    mask=nn.make_causal_mask(\n",
    "        jnp.ones((ex_inputs.shape[0],), dtype=\"bool\"),\n",
    "        dtype=\"bool\",\n",
    "    )[:, :4, :4],\n",
    ")\n",
    "\n",
    "jnp.allclose(tx_out, flax_out, atol=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jax.tree_util.tree_map(lambda a: a.shape, tx_as_flax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_output = tx_attn.apply({\"params\": tx_params}, ex_inputs)\n",
    "tx_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flax_output = flax_attn.apply({\"params\": flax_params}, ex_inputs)\n",
    "flax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_as_flax_output = flax_attn.apply({\"params\": tx_as_flax}, ex_inputs)\n",
    "tx_as_flax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_params = reference_gpt2.variables[\"params\"][\"block_0\"][\"attn\"]\n",
    "\n",
    "\n",
    "# def transform_params(params):\n",
    "#     attn_kernel, attn_bias = params[\"c_attn\"][\"kernel\"], params[\"c_attn\"][\"bias\"]\n",
    "#     proj_kernel, proj_bias = params[\"c_proj\"][\"kernel\"], params[\"c_proj\"][\"bias\"]\n",
    "\n",
    "#     def transform_kernel(a):\n",
    "#         a = jnp.transpose(a)\n",
    "#         a = jnp.reshape(a, (768, 12, 64))\n",
    "#         return a\n",
    "\n",
    "#     def transform_bias(b):\n",
    "#         return jnp.reshape(b, (12, 64))\n",
    "\n",
    "#     q_kernel, k_kernel, v_kernel = map(\n",
    "#         transform_kernel, jnp.split(attn_kernel, 3, axis=-1)\n",
    "#     )\n",
    "#     q_bias, k_bias, v_bias = map(transform_bias, jnp.split(attn_bias, 3, axis=-1))\n",
    "\n",
    "#     out_kernel = transform_kernel(proj_kernel)\n",
    "#     out_kernel = jnp.transpose(out_kernel, (1, 2, 0))\n",
    "#     # out_bias = jnp.reshape(proj_bias, (12, 64))\n",
    "\n",
    "#     # return {\n",
    "#     #     \"query\": {\"kernel\": q_kernel, \"bias\": q_bias},\n",
    "#     #     \"key\": {\"kernel\": k_kernel, \"bias\": k_bias},\n",
    "#     #     \"value\": {\"kernel\": v_kernel, \"bias\": v_bias},\n",
    "#     #     \"out\": {\"kernel\": out_kernel, \"bias\": proj_bias},\n",
    "#     # }\n",
    "#     return {\n",
    "#         \"query\": {\"kernel\": jnp.ones_like(q_kernel), \"bias\": jnp.zeros_like(q_bias)},\n",
    "#         \"key\": {\"kernel\": jnp.ones_like(k_kernel), \"bias\": jnp.zeros_like(k_bias)},\n",
    "#         \"value\": {\"kernel\": jnp.ones_like(v_kernel), \"bias\": jnp.zeros_like(v_bias)},\n",
    "#         \"out\": {\"kernel\": jnp.ones_like(out_kernel), \"bias\": jnp.zeros_like(proj_bias)},\n",
    "#     }\n",
    "\n",
    "\n",
    "# def block_params(params):\n",
    "#     # return {\n",
    "#     #     \"c_attn\": {\n",
    "#     #         \"kernel\": params[\"c_attn\"][\"kernel\"],\n",
    "#     #         \"bias\": params[\"c_attn\"][\"kernel\"],\n",
    "#     #     },\n",
    "#     #     \"c_proj\": {\n",
    "#     #         \"kernel\": params[\"c_proj\"][\"kernel\"],\n",
    "#     #         \"bias\": params[\"c_proj\"][\"bias\"],\n",
    "#     #     },\n",
    "#     # }\n",
    "#     return {\n",
    "#         \"query\": {\n",
    "#             \"kernel\": jnp.ones((768, 12, 64)),\n",
    "#             \"bias\": jnp.zeros((12, 64)),\n",
    "#         },\n",
    "#         \"key\": {\n",
    "#             \"kernel\": jnp.ones((768, 12, 64)),\n",
    "#             \"bias\": jnp.zeros((12, 64)),\n",
    "#         },\n",
    "#         \"value\": {\n",
    "#             \"kernel\": jnp.ones((768, 12, 64)),\n",
    "#             \"bias\": jnp.zeros((12, 64)),\n",
    "#         },\n",
    "#         \"proj\": {\n",
    "#             \"kernel\": jnp.ones_like(params[\"c_proj\"][\"kernel\"]),\n",
    "#             \"bias\": jnp.zeros_like(params[\"c_proj\"][\"bias\"]),\n",
    "#         },\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flax_variables = {\"params\": transform_params(test_params)}\n",
    "# flax_attn.apply(flax_variables, ex_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tx_attn.apply({\"params\": block_params(test_params)}, ex_inputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b805f1a24a146351cd8dedea7ba6f2d165b63b34e3029cd12d1a8f2ce65f719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
